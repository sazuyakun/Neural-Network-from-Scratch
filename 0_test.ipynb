{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Neural Network\n",
    "A neural network consists of:\n",
    "\n",
    "* `Layers`: Each layer has weights, biases, and an activation function.\n",
    "* `Forward Propagation`: Computes the output by passing input through the layers.\n",
    "* `Loss Function`: Measures the error between predicted and true outputs (e.g., cross-entropy for classification, MSE for regression).\n",
    "* `Backpropagation`: Computes gradients of the loss with respect to weights and biases.\n",
    "* `Gradient Descent`: Updates weights and biases to minimize the loss.\n",
    "\n",
    "We’ll implement a fully connected (dense) neural network with customizable layers and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing the Code\n",
    "File 1: `layers.py`\n",
    "This file defines a Layer class to represent a single layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The Layer class initializes weights and biases randomly.\n",
    "* `forward`: Computes the output of the layer (linear transformation + activation).\n",
    "* `backward`: Computes gradients and updates weights/biases using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation = activation  # Activation function (e.g., 'relu', 'sigmoid', 'softmax')\n",
    "        # Store intermediate values for backpropagation\n",
    "        self.input = None\n",
    "        self.z = None  # Pre-activation output\n",
    "        self.a = None  # Post-activation output\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass: X is the input (batch_size, input_size)\n",
    "        self.input = X\n",
    "        self.z = np.dot(X, self.weights) + self.biases  # Linear transformation\n",
    "        if self.activation == 'relu':\n",
    "            self.a = relu(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.a = sigmoid(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = softmax(self.z)\n",
    "        else:\n",
    "            self.a = self.z  # No activation\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        # Backward pass: delta is the gradient from the next layer\n",
    "        if self.activation == 'relu':\n",
    "            delta = delta * relu_derivative(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            delta = delta * sigmoid_derivative(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Softmax derivative is handled in the loss function (cross-entropy)\n",
    "            pass\n",
    "\n",
    "        # Compute gradients\n",
    "        dW = np.dot(self.input.T, delta)  # Gradient w.r.t weights\n",
    "        db = np.sum(delta, axis=0, keepdims=True)  # Gradient w.r.t biases\n",
    "        dX = np.dot(delta, self.weights.T)  # Gradient w.r.t input (for previous layer)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [[ 0. -1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1.  0. -1.]]\n",
      "(5, 3)\n",
      "\n",
      "bias: [[1. 1. 1.]]\n",
      "(1, 3)\n",
      "\n",
      "X: [[ 0.  0. -1. -1.  1.]\n",
      " [-1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  0.]\n",
      " [ 0.  0. -1.  0. -2.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [-1.  0. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1.]\n",
      " [ 0. -2.  0.  0.  0.]\n",
      " [-1.  0. -1.  0. -1.]]\n",
      "(10, 5)\n",
      "\n",
      "z: [[2. 3. 0.]\n",
      " [2. 3. 2.]\n",
      " [1. 1. 1.]\n",
      " [3. 3. 1.]\n",
      " [4. 2. 3.]\n",
      " [1. 1. 1.]\n",
      " [3. 4. 2.]\n",
      " [5. 3. 2.]\n",
      " [3. 1. 1.]\n",
      " [3. 3. 3.]]\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.random.randn(5, 3) // 2\n",
    "bias = np.ones((1, 3))\n",
    "X = np.random.randn(10, 5) // 2 # (batch_size, input_size)\n",
    "z = np.dot(X, weights) + bias # Linear transformation\n",
    "print(f\"weights: {weights}\")\n",
    "print(weights.shape)\n",
    "print(f\"\\nbias: {bias}\")\n",
    "print(bias.shape)\n",
    "print(f\"\\nX: {X}\")\n",
    "print(X.shape)\n",
    "print(f\"\\nz: {z}\")\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[0 0 3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "delta = np.array([1, 2, 3])\n",
    "z = np.array([-1, 0, 1])\n",
    "delta = delta * relu_derivative(z)\n",
    "print(relu_derivative(z))\n",
    "print(delta)\n",
    "\n",
    "db = np.sum(delta, axis=0, keepdims=True)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, -1,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3]])\n",
    "x - np.max(x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 2: `activations.py`\n",
    "\n",
    "Move the activation functions and their derivatives into a separate file for reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 3: `model.py`\n",
    "\n",
    "This file defines the NeuralNetwork class, which combines layers and implements training logic.\n",
    "### Explanation:\n",
    "\n",
    "* `add_layer`: Adds a layer to the network.\n",
    "* `forward`: Passes input through all layers.\n",
    "* `compute_loss`: Implements cross-entropy (for classification) and MSE (for regression).\n",
    "* `backward`: Computes gradients and updates weights.\n",
    "* `train`: Runs the training loop with forward and backward passes.\n",
    "\n",
    "### When to use which loss:\n",
    "\n",
    "* Cross-entropy: Use for classification (e.g., Iris, MNIST) where outputs are probabilities.\n",
    "* MSE: Use for regression where outputs are continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import Layer\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all layers\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, loss_type='cross_entropy'):\n",
    "        # Compute loss\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        if loss_type == 'cross_entropy':\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "        elif loss_type == 'mse':\n",
    "            loss = np.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_pred, y_true, learning_rate, loss_type='cross_entropy'):\n",
    "        # Backward propagation\n",
    "        if loss_type == 'cross_entropy':\n",
    "            # For softmax + cross-entropy, gradient is (y_pred - y_true)\n",
    "            delta = y_pred - y_true\n",
    "        elif loss_type == 'mse':\n",
    "            delta = 2 * (y_pred - y_true)  # Derivative of MSE\n",
    "\n",
    "        # Propagate gradient backward through layers\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta, learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate, loss_type='cross_entropy', verbose=True):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y_true=y, loss_type=loss_type)\n",
    "            losses.append(loss)\n",
    "            # Backward pass\n",
    "            self.backward(y_pred, y, learning_rate, loss_type)\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return class predictions (argmax for classification)\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.randint(0, 10, (10, 5))\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "target_column = 'E'\n",
    "\n",
    "X = df.drop(target_column, axis=1).values\n",
    "y = df[target_column].values\n",
    "\n",
    "y_unique = np.unique(y)\n",
    "\n",
    "y_one_hot = np.zeros((y.shape[0], len(y_unique)))\n",
    "for i, label in enumerate(y_unique):\n",
    "    y_one_hot[y == label, i] = 1\n",
    "y = y_one_hot\n",
    "y = np.argmax(y, axis=1)\n",
    "y_new = np.random.randn(y.shape[0])*10 // 6\n",
    "y_new = np.array([int(i) for i in y_new])\n",
    "\n",
    "np.sum(y_new == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 4: `utils.py`\n",
    "\n",
    "This file handles data loading, preprocessing, evaluation, and visualization.\n",
    "### Explanation:\n",
    "\n",
    "* `load_data`: Loads a CSV and optionally one-hot encodes the labels.\n",
    "* `preprocess_data`: Normalizes features and splits data.\n",
    "* `accuracy`, `confusion_matrix`, `precision_recall`: Evaluation metrics.\n",
    "* `save_model`, `load_model`: Save/load weights using NumPy.\n",
    "* `plot_metrics`: Visualizes training loss (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_path, target_column, one_hot=True):\n",
    "    # Load CSV with Pandas\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=[target_column]).values  # Features\n",
    "    y = data[target_column].values  # Labels\n",
    "\n",
    "    # One-hot encode labels if classification\n",
    "    if one_hot:\n",
    "        y_unique = np.unique(y)\n",
    "        y_one_hot = np.zeros((y.shape[0], len(y_unique)))\n",
    "        for i, label in enumerate(y):\n",
    "            y_one_hot[i, label] = 1\n",
    "        y = y_one_hot\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Accuracy for classification\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred == y_true_labels)\n",
    "\n",
    "def confusion_matrix(y_pred, y_true):\n",
    "    # Simple confusion matrix\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes))\n",
    "    for t, p in zip(y_true_labels, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def precision_recall(y_pred, y_true):\n",
    "    # Precision and recall per class\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    return precision, recall\n",
    "\n",
    "def save_model(model, filename):\n",
    "    # Save weights and biases using NumPy\n",
    "    params = [{'weights': layer.weights, 'biases': layer.biases} for layer in model.layers]\n",
    "    np.save(filename, params)\n",
    "\n",
    "def load_model(filename):\n",
    "    # Load weights and biases\n",
    "    params = np.load(filename, allow_pickle=True)\n",
    "    nn = NeuralNetwork()\n",
    "    for param in params:\n",
    "        layer = Layer(param['weights'].shape[0], param['weights'].shape[1])\n",
    "        layer.weights = param['weights']\n",
    "        layer.biases = param['biases']\n",
    "        nn.add_layer(layer)\n",
    "    return nn\n",
    "\n",
    "def plot_metrics(losses, title=\"Training Loss\"):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NeuralNetwork\n",
    "from layers import Layer\n",
    "from utils import load_data, preprocess_data, accuracy, confusion_matrix, precision_recall, save_model, plot_metrics, load_model\n",
    "\n",
    "X, y = load_data('iris.csv', target_column='species', one_hot=True)\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add_layer(Layer(input_size=X.shape[1], output_size=10, activation='relu'))\n",
    "nn.add_layer(Layer(input_size=10, output_size=y_train.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8418.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Epoch: 1\n",
      "Loss:  1.7632\n",
      "==========\n",
      "Epoch: 2\n",
      "Loss:  1.0424\n",
      "==========\n",
      "Epoch: 3\n",
      "Loss:  0.8220\n",
      "==========\n",
      "Epoch: 4\n",
      "Loss:  0.7342\n",
      "==========\n",
      "Epoch: 5\n",
      "Loss:  0.6788\n",
      "==========\n",
      "Epoch: 6\n",
      "Loss:  0.6386\n",
      "==========\n",
      "Epoch: 7\n",
      "Loss:  0.6078\n",
      "==========\n",
      "Epoch: 8\n",
      "Loss:  0.5832\n",
      "==========\n",
      "Epoch: 9\n",
      "Loss:  0.5631\n",
      "==========\n",
      "Epoch: 10\n",
      "Loss:  0.5471\n",
      "==========\n",
      "Epoch: 11\n",
      "Loss:  0.5337\n",
      "==========\n",
      "Epoch: 12\n",
      "Loss:  0.5218\n",
      "==========\n",
      "Epoch: 13\n",
      "Loss:  0.5111\n",
      "==========\n",
      "Epoch: 14\n",
      "Loss:  0.5016\n",
      "==========\n",
      "Epoch: 15\n",
      "Loss:  0.4929\n",
      "==========\n",
      "Epoch: 16\n",
      "Loss:  0.4853\n",
      "==========\n",
      "Epoch: 17\n",
      "Loss:  0.4781\n",
      "==========\n",
      "Epoch: 18\n",
      "Loss:  0.4715\n",
      "==========\n",
      "Epoch: 19\n",
      "Loss:  0.4651\n",
      "==========\n",
      "Epoch: 20\n",
      "Loss:  0.4588\n",
      "==========\n",
      "Epoch: 21\n",
      "Loss:  0.4529\n",
      "==========\n",
      "Epoch: 22\n",
      "Loss:  0.4472\n",
      "==========\n",
      "Epoch: 23\n",
      "Loss:  0.4417\n",
      "==========\n",
      "Epoch: 24\n",
      "Loss:  0.4363\n",
      "==========\n",
      "Epoch: 25\n",
      "Loss:  0.4312\n",
      "==========\n",
      "Epoch: 26\n",
      "Loss:  0.4262\n",
      "==========\n",
      "Epoch: 27\n",
      "Loss:  0.4212\n",
      "==========\n",
      "Epoch: 28\n",
      "Loss:  0.4165\n",
      "==========\n",
      "Epoch: 29\n",
      "Loss:  0.4116\n",
      "==========\n",
      "Epoch: 30\n",
      "Loss:  0.4064\n",
      "==========\n",
      "Epoch: 31\n",
      "Loss:  0.4015\n",
      "==========\n",
      "Epoch: 32\n",
      "Loss:  0.3969\n",
      "==========\n",
      "Epoch: 33\n",
      "Loss:  0.3925\n",
      "==========\n",
      "Epoch: 34\n",
      "Loss:  0.3884\n",
      "==========\n",
      "Epoch: 35\n",
      "Loss:  0.3844\n",
      "==========\n",
      "Epoch: 36\n",
      "Loss:  0.3803\n",
      "==========\n",
      "Epoch: 37\n",
      "Loss:  0.3761\n",
      "==========\n",
      "Epoch: 38\n",
      "Loss:  0.3720\n",
      "==========\n",
      "Epoch: 39\n",
      "Loss:  0.3681\n",
      "==========\n",
      "Epoch: 40\n",
      "Loss:  0.3643\n",
      "==========\n",
      "Epoch: 41\n",
      "Loss:  0.3607\n",
      "==========\n",
      "Epoch: 42\n",
      "Loss:  0.3571\n",
      "==========\n",
      "Epoch: 43\n",
      "Loss:  0.3536\n",
      "==========\n",
      "Epoch: 44\n",
      "Loss:  0.3503\n",
      "==========\n",
      "Epoch: 45\n",
      "Loss:  0.3469\n",
      "==========\n",
      "Epoch: 46\n",
      "Loss:  0.3435\n",
      "==========\n",
      "Epoch: 47\n",
      "Loss:  0.3401\n",
      "==========\n",
      "Epoch: 48\n",
      "Loss:  0.3369\n",
      "==========\n",
      "Epoch: 49\n",
      "Loss:  0.3337\n",
      "==========\n",
      "Epoch: 50\n",
      "Loss:  0.3305\n",
      "==========\n",
      "Epoch: 51\n",
      "Loss:  0.3274\n",
      "==========\n",
      "Epoch: 52\n",
      "Loss:  0.3244\n",
      "==========\n",
      "Epoch: 53\n",
      "Loss:  0.3215\n",
      "==========\n",
      "Epoch: 54\n",
      "Loss:  0.3187\n",
      "==========\n",
      "Epoch: 55\n",
      "Loss:  0.3159\n",
      "==========\n",
      "Epoch: 56\n",
      "Loss:  0.3132\n",
      "==========\n",
      "Epoch: 57\n",
      "Loss:  0.3105\n",
      "==========\n",
      "Epoch: 58\n",
      "Loss:  0.3080\n",
      "==========\n",
      "Epoch: 59\n",
      "Loss:  0.3055\n",
      "==========\n",
      "Epoch: 60\n",
      "Loss:  0.3031\n",
      "==========\n",
      "Epoch: 61\n",
      "Loss:  0.3007\n",
      "==========\n",
      "Epoch: 62\n",
      "Loss:  0.2983\n",
      "==========\n",
      "Epoch: 63\n",
      "Loss:  0.2960\n",
      "==========\n",
      "Epoch: 64\n",
      "Loss:  0.2936\n",
      "==========\n",
      "Epoch: 65\n",
      "Loss:  0.2914\n",
      "==========\n",
      "Epoch: 66\n",
      "Loss:  0.2892\n",
      "==========\n",
      "Epoch: 67\n",
      "Loss:  0.2870\n",
      "==========\n",
      "Epoch: 68\n",
      "Loss:  0.2848\n",
      "==========\n",
      "Epoch: 69\n",
      "Loss:  0.2827\n",
      "==========\n",
      "Epoch: 70\n",
      "Loss:  0.2806\n",
      "==========\n",
      "Epoch: 71\n",
      "Loss:  0.2786\n",
      "==========\n",
      "Epoch: 72\n",
      "Loss:  0.2765\n",
      "==========\n",
      "Epoch: 73\n",
      "Loss:  0.2745\n",
      "==========\n",
      "Epoch: 74\n",
      "Loss:  0.2724\n",
      "==========\n",
      "Epoch: 75\n",
      "Loss:  0.2703\n",
      "==========\n",
      "Epoch: 76\n",
      "Loss:  0.2683\n",
      "==========\n",
      "Epoch: 77\n",
      "Loss:  0.2663\n",
      "==========\n",
      "Epoch: 78\n",
      "Loss:  0.2643\n",
      "==========\n",
      "Epoch: 79\n",
      "Loss:  0.2623\n",
      "==========\n",
      "Epoch: 80\n",
      "Loss:  0.2604\n",
      "==========\n",
      "Epoch: 81\n",
      "Loss:  0.2585\n",
      "==========\n",
      "Epoch: 82\n",
      "Loss:  0.2566\n",
      "==========\n",
      "Epoch: 83\n",
      "Loss:  0.2547\n",
      "==========\n",
      "Epoch: 84\n",
      "Loss:  0.2525\n",
      "==========\n",
      "Epoch: 85\n",
      "Loss:  0.2505\n",
      "==========\n",
      "Epoch: 86\n",
      "Loss:  0.2484\n",
      "==========\n",
      "Epoch: 87\n",
      "Loss:  0.2464\n",
      "==========\n",
      "Epoch: 88\n",
      "Loss:  0.2444\n",
      "==========\n",
      "Epoch: 89\n",
      "Loss:  0.2424\n",
      "==========\n",
      "Epoch: 90\n",
      "Loss:  0.2405\n",
      "==========\n",
      "Epoch: 91\n",
      "Loss:  0.2385\n",
      "==========\n",
      "Epoch: 92\n",
      "Loss:  0.2366\n",
      "==========\n",
      "Epoch: 93\n",
      "Loss:  0.2348\n",
      "==========\n",
      "Epoch: 94\n",
      "Loss:  0.2329\n",
      "==========\n",
      "Epoch: 95\n",
      "Loss:  0.2311\n",
      "==========\n",
      "Epoch: 96\n",
      "Loss:  0.2294\n",
      "==========\n",
      "Epoch: 97\n",
      "Loss:  0.2276\n",
      "==========\n",
      "Epoch: 98\n",
      "Loss:  0.2258\n",
      "==========\n",
      "Epoch: 99\n",
      "Loss:  0.2240\n",
      "==========\n",
      "Epoch: 100\n",
      "Loss:  0.2223\n",
      "==========\n",
      "Epoch: 101\n",
      "Loss:  0.2206\n",
      "==========\n",
      "Epoch: 102\n",
      "Loss:  0.2189\n",
      "==========\n",
      "Epoch: 103\n",
      "Loss:  0.2172\n",
      "==========\n",
      "Epoch: 104\n",
      "Loss:  0.2155\n",
      "==========\n",
      "Epoch: 105\n",
      "Loss:  0.2138\n",
      "==========\n",
      "Epoch: 106\n",
      "Loss:  0.2122\n",
      "==========\n",
      "Epoch: 107\n",
      "Loss:  0.2105\n",
      "==========\n",
      "Epoch: 108\n",
      "Loss:  0.2090\n",
      "==========\n",
      "Epoch: 109\n",
      "Loss:  0.2074\n",
      "==========\n",
      "Epoch: 110\n",
      "Loss:  0.2059\n",
      "==========\n",
      "Epoch: 111\n",
      "Loss:  0.2044\n",
      "==========\n",
      "Epoch: 112\n",
      "Loss:  0.2029\n",
      "==========\n",
      "Epoch: 113\n",
      "Loss:  0.2014\n",
      "==========\n",
      "Epoch: 114\n",
      "Loss:  0.2000\n",
      "==========\n",
      "Epoch: 115\n",
      "Loss:  0.1986\n",
      "==========\n",
      "Epoch: 116\n",
      "Loss:  0.1972\n",
      "==========\n",
      "Epoch: 117\n",
      "Loss:  0.1959\n",
      "==========\n",
      "Epoch: 118\n",
      "Loss:  0.1946\n",
      "==========\n",
      "Epoch: 119\n",
      "Loss:  0.1933\n",
      "==========\n",
      "Epoch: 120\n",
      "Loss:  0.1920\n",
      "==========\n",
      "Epoch: 121\n",
      "Loss:  0.1907\n",
      "==========\n",
      "Epoch: 122\n",
      "Loss:  0.1895\n",
      "==========\n",
      "Epoch: 123\n",
      "Loss:  0.1883\n",
      "==========\n",
      "Epoch: 124\n",
      "Loss:  0.1870\n",
      "==========\n",
      "Epoch: 125\n",
      "Loss:  0.1858\n",
      "==========\n",
      "Epoch: 126\n",
      "Loss:  0.1846\n",
      "==========\n",
      "Epoch: 127\n",
      "Loss:  0.1834\n",
      "==========\n",
      "Epoch: 128\n",
      "Loss:  0.1821\n",
      "==========\n",
      "Epoch: 129\n",
      "Loss:  0.1809\n",
      "==========\n",
      "Epoch: 130\n",
      "Loss:  0.1798\n",
      "==========\n",
      "Epoch: 131\n",
      "Loss:  0.1786\n",
      "==========\n",
      "Epoch: 132\n",
      "Loss:  0.1774\n",
      "==========\n",
      "Epoch: 133\n",
      "Loss:  0.1762\n",
      "==========\n",
      "Epoch: 134\n",
      "Loss:  0.1750\n",
      "==========\n",
      "Epoch: 135\n",
      "Loss:  0.1738\n",
      "==========\n",
      "Epoch: 136\n",
      "Loss:  0.1725\n",
      "==========\n",
      "Epoch: 137\n",
      "Loss:  0.1713\n",
      "==========\n",
      "Epoch: 138\n",
      "Loss:  0.1700\n",
      "==========\n",
      "Epoch: 139\n",
      "Loss:  0.1688\n",
      "==========\n",
      "Epoch: 140\n",
      "Loss:  0.1676\n",
      "==========\n",
      "Epoch: 141\n",
      "Loss:  0.1665\n",
      "==========\n",
      "Epoch: 142\n",
      "Loss:  0.1653\n",
      "==========\n",
      "Epoch: 143\n",
      "Loss:  0.1642\n",
      "==========\n",
      "Epoch: 144\n",
      "Loss:  0.1631\n",
      "==========\n",
      "Epoch: 145\n",
      "Loss:  0.1620\n",
      "==========\n",
      "Epoch: 146\n",
      "Loss:  0.1610\n",
      "==========\n",
      "Epoch: 147\n",
      "Loss:  0.1599\n",
      "==========\n",
      "Epoch: 148\n",
      "Loss:  0.1589\n",
      "==========\n",
      "Epoch: 149\n",
      "Loss:  0.1579\n",
      "==========\n",
      "Epoch: 150\n",
      "Loss:  0.1569\n",
      "==========\n",
      "Epoch: 151\n",
      "Loss:  0.1558\n",
      "==========\n",
      "Epoch: 152\n",
      "Loss:  0.1548\n",
      "==========\n",
      "Epoch: 153\n",
      "Loss:  0.1538\n",
      "==========\n",
      "Epoch: 154\n",
      "Loss:  0.1528\n",
      "==========\n",
      "Epoch: 155\n",
      "Loss:  0.1518\n",
      "==========\n",
      "Epoch: 156\n",
      "Loss:  0.1508\n",
      "==========\n",
      "Epoch: 157\n",
      "Loss:  0.1497\n",
      "==========\n",
      "Epoch: 158\n",
      "Loss:  0.1487\n",
      "==========\n",
      "Epoch: 159\n",
      "Loss:  0.1476\n",
      "==========\n",
      "Epoch: 160\n",
      "Loss:  0.1466\n",
      "==========\n",
      "Epoch: 161\n",
      "Loss:  0.1456\n",
      "==========\n",
      "Epoch: 162\n",
      "Loss:  0.1446\n",
      "==========\n",
      "Epoch: 163\n",
      "Loss:  0.1436\n",
      "==========\n",
      "Epoch: 164\n",
      "Loss:  0.1426\n",
      "==========\n",
      "Epoch: 165\n",
      "Loss:  0.1416\n",
      "==========\n",
      "Epoch: 166\n",
      "Loss:  0.1407\n",
      "==========\n",
      "Epoch: 167\n",
      "Loss:  0.1397\n",
      "==========\n",
      "Epoch: 168\n",
      "Loss:  0.1388\n",
      "==========\n",
      "Epoch: 169\n",
      "Loss:  0.1379\n",
      "==========\n",
      "Epoch: 170\n",
      "Loss:  0.1371\n",
      "==========\n",
      "Epoch: 171\n",
      "Loss:  0.1362\n",
      "==========\n",
      "Epoch: 172\n",
      "Loss:  0.1354\n",
      "==========\n",
      "Epoch: 173\n",
      "Loss:  0.1345\n",
      "==========\n",
      "Epoch: 174\n",
      "Loss:  0.1337\n",
      "==========\n",
      "Epoch: 175\n",
      "Loss:  0.1329\n",
      "==========\n",
      "Epoch: 176\n",
      "Loss:  0.1321\n",
      "==========\n",
      "Epoch: 177\n",
      "Loss:  0.1314\n",
      "==========\n",
      "Epoch: 178\n",
      "Loss:  0.1306\n",
      "==========\n",
      "Epoch: 179\n",
      "Loss:  0.1298\n",
      "==========\n",
      "Epoch: 180\n",
      "Loss:  0.1291\n",
      "==========\n",
      "Epoch: 181\n",
      "Loss:  0.1283\n",
      "==========\n",
      "Epoch: 182\n",
      "Loss:  0.1276\n",
      "==========\n",
      "Epoch: 183\n",
      "Loss:  0.1269\n",
      "==========\n",
      "Epoch: 184\n",
      "Loss:  0.1262\n",
      "==========\n",
      "Epoch: 185\n",
      "Loss:  0.1256\n",
      "==========\n",
      "Epoch: 186\n",
      "Loss:  0.1250\n",
      "==========\n",
      "Epoch: 187\n",
      "Loss:  0.1244\n",
      "==========\n",
      "Epoch: 188\n",
      "Loss:  0.1238\n",
      "==========\n",
      "Epoch: 189\n",
      "Loss:  0.1232\n",
      "==========\n",
      "Epoch: 190\n",
      "Loss:  0.1226\n",
      "==========\n",
      "Epoch: 191\n",
      "Loss:  0.1220\n",
      "==========\n",
      "Epoch: 192\n",
      "Loss:  0.1214\n",
      "==========\n",
      "Epoch: 193\n",
      "Loss:  0.1208\n",
      "==========\n",
      "Epoch: 194\n",
      "Loss:  0.1203\n",
      "==========\n",
      "Epoch: 195\n",
      "Loss:  0.1197\n",
      "==========\n",
      "Epoch: 196\n",
      "Loss:  0.1192\n",
      "==========\n",
      "Epoch: 197\n",
      "Loss:  0.1187\n",
      "==========\n",
      "Epoch: 198\n",
      "Loss:  0.1182\n",
      "==========\n",
      "Epoch: 199\n",
      "Loss:  0.1177\n",
      "==========\n",
      "Epoch: 200\n",
      "Loss:  0.1172\n",
      "==========\n",
      "Epoch: 201\n",
      "Loss:  0.1167\n",
      "==========\n",
      "Epoch: 202\n",
      "Loss:  0.1162\n",
      "==========\n",
      "Epoch: 203\n",
      "Loss:  0.1157\n",
      "==========\n",
      "Epoch: 204\n",
      "Loss:  0.1152\n",
      "==========\n",
      "Epoch: 205\n",
      "Loss:  0.1147\n",
      "==========\n",
      "Epoch: 206\n",
      "Loss:  0.1142\n",
      "==========\n",
      "Epoch: 207\n",
      "Loss:  0.1138\n",
      "==========\n",
      "Epoch: 208\n",
      "Loss:  0.1133\n",
      "==========\n",
      "Epoch: 209\n",
      "Loss:  0.1128\n",
      "==========\n",
      "Epoch: 210\n",
      "Loss:  0.1124\n",
      "==========\n",
      "Epoch: 211\n",
      "Loss:  0.1119\n",
      "==========\n",
      "Epoch: 212\n",
      "Loss:  0.1115\n",
      "==========\n",
      "Epoch: 213\n",
      "Loss:  0.1110\n",
      "==========\n",
      "Epoch: 214\n",
      "Loss:  0.1106\n",
      "==========\n",
      "Epoch: 215\n",
      "Loss:  0.1101\n",
      "==========\n",
      "Epoch: 216\n",
      "Loss:  0.1097\n",
      "==========\n",
      "Epoch: 217\n",
      "Loss:  0.1093\n",
      "==========\n",
      "Epoch: 218\n",
      "Loss:  0.1088\n",
      "==========\n",
      "Epoch: 219\n",
      "Loss:  0.1084\n",
      "==========\n",
      "Epoch: 220\n",
      "Loss:  0.1080\n",
      "==========\n",
      "Epoch: 221\n",
      "Loss:  0.1076\n",
      "==========\n",
      "Epoch: 222\n",
      "Loss:  0.1072\n",
      "==========\n",
      "Epoch: 223\n",
      "Loss:  0.1067\n",
      "==========\n",
      "Epoch: 224\n",
      "Loss:  0.1063\n",
      "==========\n",
      "Epoch: 225\n",
      "Loss:  0.1059\n",
      "==========\n",
      "Epoch: 226\n",
      "Loss:  0.1055\n",
      "==========\n",
      "Epoch: 227\n",
      "Loss:  0.1052\n",
      "==========\n",
      "Epoch: 228\n",
      "Loss:  0.1048\n",
      "==========\n",
      "Epoch: 229\n",
      "Loss:  0.1044\n",
      "==========\n",
      "Epoch: 230\n",
      "Loss:  0.1040\n",
      "==========\n",
      "Epoch: 231\n",
      "Loss:  0.1036\n",
      "==========\n",
      "Epoch: 232\n",
      "Loss:  0.1033\n",
      "==========\n",
      "Epoch: 233\n",
      "Loss:  0.1029\n",
      "==========\n",
      "Epoch: 234\n",
      "Loss:  0.1025\n",
      "==========\n",
      "Epoch: 235\n",
      "Loss:  0.1022\n",
      "==========\n",
      "Epoch: 236\n",
      "Loss:  0.1018\n",
      "==========\n",
      "Epoch: 237\n",
      "Loss:  0.1015\n",
      "==========\n",
      "Epoch: 238\n",
      "Loss:  0.1011\n",
      "==========\n",
      "Epoch: 239\n",
      "Loss:  0.1008\n",
      "==========\n",
      "Epoch: 240\n",
      "Loss:  0.1005\n",
      "==========\n",
      "Epoch: 241\n",
      "Loss:  0.1001\n",
      "==========\n",
      "Epoch: 242\n",
      "Loss:  0.0998\n",
      "==========\n",
      "Epoch: 243\n",
      "Loss:  0.0995\n",
      "==========\n",
      "Epoch: 244\n",
      "Loss:  0.0991\n",
      "==========\n",
      "Epoch: 245\n",
      "Loss:  0.0988\n",
      "==========\n",
      "Epoch: 246\n",
      "Loss:  0.0985\n",
      "==========\n",
      "Epoch: 247\n",
      "Loss:  0.0981\n",
      "==========\n",
      "Epoch: 248\n",
      "Loss:  0.0978\n",
      "==========\n",
      "Epoch: 249\n",
      "Loss:  0.0974\n",
      "==========\n",
      "Epoch: 250\n",
      "Loss:  0.0971\n",
      "==========\n",
      "Epoch: 251\n",
      "Loss:  0.0968\n",
      "==========\n",
      "Epoch: 252\n",
      "Loss:  0.0965\n",
      "==========\n",
      "Epoch: 253\n",
      "Loss:  0.0961\n",
      "==========\n",
      "Epoch: 254\n",
      "Loss:  0.0958\n",
      "==========\n",
      "Epoch: 255\n",
      "Loss:  0.0955\n",
      "==========\n",
      "Epoch: 256\n",
      "Loss:  0.0952\n",
      "==========\n",
      "Epoch: 257\n",
      "Loss:  0.0949\n",
      "==========\n",
      "Epoch: 258\n",
      "Loss:  0.0946\n",
      "==========\n",
      "Epoch: 259\n",
      "Loss:  0.0943\n",
      "==========\n",
      "Epoch: 260\n",
      "Loss:  0.0940\n",
      "==========\n",
      "Epoch: 261\n",
      "Loss:  0.0937\n",
      "==========\n",
      "Epoch: 262\n",
      "Loss:  0.0934\n",
      "==========\n",
      "Epoch: 263\n",
      "Loss:  0.0931\n",
      "==========\n",
      "Epoch: 264\n",
      "Loss:  0.0928\n",
      "==========\n",
      "Epoch: 265\n",
      "Loss:  0.0926\n",
      "==========\n",
      "Epoch: 266\n",
      "Loss:  0.0923\n",
      "==========\n",
      "Epoch: 267\n",
      "Loss:  0.0920\n",
      "==========\n",
      "Epoch: 268\n",
      "Loss:  0.0917\n",
      "==========\n",
      "Epoch: 269\n",
      "Loss:  0.0915\n",
      "==========\n",
      "Epoch: 270\n",
      "Loss:  0.0912\n",
      "==========\n",
      "Epoch: 271\n",
      "Loss:  0.0910\n",
      "==========\n",
      "Epoch: 272\n",
      "Loss:  0.0907\n",
      "==========\n",
      "Epoch: 273\n",
      "Loss:  0.0905\n",
      "==========\n",
      "Epoch: 274\n",
      "Loss:  0.0902\n",
      "==========\n",
      "Epoch: 275\n",
      "Loss:  0.0900\n",
      "==========\n",
      "Epoch: 276\n",
      "Loss:  0.0897\n",
      "==========\n",
      "Epoch: 277\n",
      "Loss:  0.0895\n",
      "==========\n",
      "Epoch: 278\n",
      "Loss:  0.0893\n",
      "==========\n",
      "Epoch: 279\n",
      "Loss:  0.0890\n",
      "==========\n",
      "Epoch: 280\n",
      "Loss:  0.0888\n",
      "==========\n",
      "Epoch: 281\n",
      "Loss:  0.0885\n",
      "==========\n",
      "Epoch: 282\n",
      "Loss:  0.0883\n",
      "==========\n",
      "Epoch: 283\n",
      "Loss:  0.0880\n",
      "==========\n",
      "Epoch: 284\n",
      "Loss:  0.0878\n",
      "==========\n",
      "Epoch: 285\n",
      "Loss:  0.0875\n",
      "==========\n",
      "Epoch: 286\n",
      "Loss:  0.0873\n",
      "==========\n",
      "Epoch: 287\n",
      "Loss:  0.0870\n",
      "==========\n",
      "Epoch: 288\n",
      "Loss:  0.0868\n",
      "==========\n",
      "Epoch: 289\n",
      "Loss:  0.0865\n",
      "==========\n",
      "Epoch: 290\n",
      "Loss:  0.0863\n",
      "==========\n",
      "Epoch: 291\n",
      "Loss:  0.0861\n",
      "==========\n",
      "Epoch: 292\n",
      "Loss:  0.0858\n",
      "==========\n",
      "Epoch: 293\n",
      "Loss:  0.0856\n",
      "==========\n",
      "Epoch: 294\n",
      "Loss:  0.0854\n",
      "==========\n",
      "Epoch: 295\n",
      "Loss:  0.0852\n",
      "==========\n",
      "Epoch: 296\n",
      "Loss:  0.0850\n",
      "==========\n",
      "Epoch: 297\n",
      "Loss:  0.0848\n",
      "==========\n",
      "Epoch: 298\n",
      "Loss:  0.0845\n",
      "==========\n",
      "Epoch: 299\n",
      "Loss:  0.0843\n",
      "==========\n",
      "Epoch: 300\n",
      "Loss:  0.0842\n",
      "==========\n",
      "Epoch: 301\n",
      "Loss:  0.0840\n",
      "==========\n",
      "Epoch: 302\n",
      "Loss:  0.0838\n",
      "==========\n",
      "Epoch: 303\n",
      "Loss:  0.0836\n",
      "==========\n",
      "Epoch: 304\n",
      "Loss:  0.0834\n",
      "==========\n",
      "Epoch: 305\n",
      "Loss:  0.0832\n",
      "==========\n",
      "Epoch: 306\n",
      "Loss:  0.0830\n",
      "==========\n",
      "Epoch: 307\n",
      "Loss:  0.0828\n",
      "==========\n",
      "Epoch: 308\n",
      "Loss:  0.0826\n",
      "==========\n",
      "Epoch: 309\n",
      "Loss:  0.0825\n",
      "==========\n",
      "Epoch: 310\n",
      "Loss:  0.0823\n",
      "==========\n",
      "Epoch: 311\n",
      "Loss:  0.0821\n",
      "==========\n",
      "Epoch: 312\n",
      "Loss:  0.0819\n",
      "==========\n",
      "Epoch: 313\n",
      "Loss:  0.0817\n",
      "==========\n",
      "Epoch: 314\n",
      "Loss:  0.0816\n",
      "==========\n",
      "Epoch: 315\n",
      "Loss:  0.0814\n",
      "==========\n",
      "Epoch: 316\n",
      "Loss:  0.0812\n",
      "==========\n",
      "Epoch: 317\n",
      "Loss:  0.0810\n",
      "==========\n",
      "Epoch: 318\n",
      "Loss:  0.0809\n",
      "==========\n",
      "Epoch: 319\n",
      "Loss:  0.0807\n",
      "==========\n",
      "Epoch: 320\n",
      "Loss:  0.0805\n",
      "==========\n",
      "Epoch: 321\n",
      "Loss:  0.0803\n",
      "==========\n",
      "Epoch: 322\n",
      "Loss:  0.0802\n",
      "==========\n",
      "Epoch: 323\n",
      "Loss:  0.0800\n",
      "==========\n",
      "Epoch: 324\n",
      "Loss:  0.0798\n",
      "==========\n",
      "Epoch: 325\n",
      "Loss:  0.0797\n",
      "==========\n",
      "Epoch: 326\n",
      "Loss:  0.0795\n",
      "==========\n",
      "Epoch: 327\n",
      "Loss:  0.0794\n",
      "==========\n",
      "Epoch: 328\n",
      "Loss:  0.0792\n",
      "==========\n",
      "Epoch: 329\n",
      "Loss:  0.0790\n",
      "==========\n",
      "Epoch: 330\n",
      "Loss:  0.0789\n",
      "==========\n",
      "Epoch: 331\n",
      "Loss:  0.0787\n",
      "==========\n",
      "Epoch: 332\n",
      "Loss:  0.0786\n",
      "==========\n",
      "Epoch: 333\n",
      "Loss:  0.0784\n",
      "==========\n",
      "Epoch: 334\n",
      "Loss:  0.0783\n",
      "==========\n",
      "Epoch: 335\n",
      "Loss:  0.0781\n",
      "==========\n",
      "Epoch: 336\n",
      "Loss:  0.0780\n",
      "==========\n",
      "Epoch: 337\n",
      "Loss:  0.0778\n",
      "==========\n",
      "Epoch: 338\n",
      "Loss:  0.0777\n",
      "==========\n",
      "Epoch: 339\n",
      "Loss:  0.0775\n",
      "==========\n",
      "Epoch: 340\n",
      "Loss:  0.0774\n",
      "==========\n",
      "Epoch: 341\n",
      "Loss:  0.0772\n",
      "==========\n",
      "Epoch: 342\n",
      "Loss:  0.0771\n",
      "==========\n",
      "Epoch: 343\n",
      "Loss:  0.0770\n",
      "==========\n",
      "Epoch: 344\n",
      "Loss:  0.0768\n",
      "==========\n",
      "Epoch: 345\n",
      "Loss:  0.0767\n",
      "==========\n",
      "Epoch: 346\n",
      "Loss:  0.0765\n",
      "==========\n",
      "Epoch: 347\n",
      "Loss:  0.0764\n",
      "==========\n",
      "Epoch: 348\n",
      "Loss:  0.0763\n",
      "==========\n",
      "Epoch: 349\n",
      "Loss:  0.0761\n",
      "==========\n",
      "Epoch: 350\n",
      "Loss:  0.0760\n",
      "==========\n",
      "Epoch: 351\n",
      "Loss:  0.0759\n",
      "==========\n",
      "Epoch: 352\n",
      "Loss:  0.0757\n",
      "==========\n",
      "Epoch: 353\n",
      "Loss:  0.0756\n",
      "==========\n",
      "Epoch: 354\n",
      "Loss:  0.0755\n",
      "==========\n",
      "Epoch: 355\n",
      "Loss:  0.0754\n",
      "==========\n",
      "Epoch: 356\n",
      "Loss:  0.0752\n",
      "==========\n",
      "Epoch: 357\n",
      "Loss:  0.0751\n",
      "==========\n",
      "Epoch: 358\n",
      "Loss:  0.0750\n",
      "==========\n",
      "Epoch: 359\n",
      "Loss:  0.0748\n",
      "==========\n",
      "Epoch: 360\n",
      "Loss:  0.0747\n",
      "==========\n",
      "Epoch: 361\n",
      "Loss:  0.0746\n",
      "==========\n",
      "Epoch: 362\n",
      "Loss:  0.0745\n",
      "==========\n",
      "Epoch: 363\n",
      "Loss:  0.0743\n",
      "==========\n",
      "Epoch: 364\n",
      "Loss:  0.0742\n",
      "==========\n",
      "Epoch: 365\n",
      "Loss:  0.0741\n",
      "==========\n",
      "Epoch: 366\n",
      "Loss:  0.0740\n",
      "==========\n",
      "Epoch: 367\n",
      "Loss:  0.0739\n",
      "==========\n",
      "Epoch: 368\n",
      "Loss:  0.0738\n",
      "==========\n",
      "Epoch: 369\n",
      "Loss:  0.0736\n",
      "==========\n",
      "Epoch: 370\n",
      "Loss:  0.0735\n",
      "==========\n",
      "Epoch: 371\n",
      "Loss:  0.0734\n",
      "==========\n",
      "Epoch: 372\n",
      "Loss:  0.0733\n",
      "==========\n",
      "Epoch: 373\n",
      "Loss:  0.0732\n",
      "==========\n",
      "Epoch: 374\n",
      "Loss:  0.0731\n",
      "==========\n",
      "Epoch: 375\n",
      "Loss:  0.0729\n",
      "==========\n",
      "Epoch: 376\n",
      "Loss:  0.0728\n",
      "==========\n",
      "Epoch: 377\n",
      "Loss:  0.0727\n",
      "==========\n",
      "Epoch: 378\n",
      "Loss:  0.0726\n",
      "==========\n",
      "Epoch: 379\n",
      "Loss:  0.0725\n",
      "==========\n",
      "Epoch: 380\n",
      "Loss:  0.0724\n",
      "==========\n",
      "Epoch: 381\n",
      "Loss:  0.0723\n",
      "==========\n",
      "Epoch: 382\n",
      "Loss:  0.0722\n",
      "==========\n",
      "Epoch: 383\n",
      "Loss:  0.0721\n",
      "==========\n",
      "Epoch: 384\n",
      "Loss:  0.0720\n",
      "==========\n",
      "Epoch: 385\n",
      "Loss:  0.0719\n",
      "==========\n",
      "Epoch: 386\n",
      "Loss:  0.0718\n",
      "==========\n",
      "Epoch: 387\n",
      "Loss:  0.0716\n",
      "==========\n",
      "Epoch: 388\n",
      "Loss:  0.0715\n",
      "==========\n",
      "Epoch: 389\n",
      "Loss:  0.0714\n",
      "==========\n",
      "Epoch: 390\n",
      "Loss:  0.0713\n",
      "==========\n",
      "Epoch: 391\n",
      "Loss:  0.0713\n",
      "==========\n",
      "Epoch: 392\n",
      "Loss:  0.0712\n",
      "==========\n",
      "Epoch: 393\n",
      "Loss:  0.0711\n",
      "==========\n",
      "Epoch: 394\n",
      "Loss:  0.0710\n",
      "==========\n",
      "Epoch: 395\n",
      "Loss:  0.0709\n",
      "==========\n",
      "Epoch: 396\n",
      "Loss:  0.0708\n",
      "==========\n",
      "Epoch: 397\n",
      "Loss:  0.0707\n",
      "==========\n",
      "Epoch: 398\n",
      "Loss:  0.0706\n",
      "==========\n",
      "Epoch: 399\n",
      "Loss:  0.0705\n",
      "==========\n",
      "Epoch: 400\n",
      "Loss:  0.0704\n",
      "==========\n",
      "Epoch: 401\n",
      "Loss:  0.0704\n",
      "==========\n",
      "Epoch: 402\n",
      "Loss:  0.0703\n",
      "==========\n",
      "Epoch: 403\n",
      "Loss:  0.0702\n",
      "==========\n",
      "Epoch: 404\n",
      "Loss:  0.0701\n",
      "==========\n",
      "Epoch: 405\n",
      "Loss:  0.0700\n",
      "==========\n",
      "Epoch: 406\n",
      "Loss:  0.0699\n",
      "==========\n",
      "Epoch: 407\n",
      "Loss:  0.0698\n",
      "==========\n",
      "Epoch: 408\n",
      "Loss:  0.0698\n",
      "==========\n",
      "Epoch: 409\n",
      "Loss:  0.0697\n",
      "==========\n",
      "Epoch: 410\n",
      "Loss:  0.0696\n",
      "==========\n",
      "Epoch: 411\n",
      "Loss:  0.0695\n",
      "==========\n",
      "Epoch: 412\n",
      "Loss:  0.0694\n",
      "==========\n",
      "Epoch: 413\n",
      "Loss:  0.0693\n",
      "==========\n",
      "Epoch: 414\n",
      "Loss:  0.0693\n",
      "==========\n",
      "Epoch: 415\n",
      "Loss:  0.0692\n",
      "==========\n",
      "Epoch: 416\n",
      "Loss:  0.0691\n",
      "==========\n",
      "Epoch: 417\n",
      "Loss:  0.0690\n",
      "==========\n",
      "Epoch: 418\n",
      "Loss:  0.0689\n",
      "==========\n",
      "Epoch: 419\n",
      "Loss:  0.0689\n",
      "==========\n",
      "Epoch: 420\n",
      "Loss:  0.0688\n",
      "==========\n",
      "Epoch: 421\n",
      "Loss:  0.0687\n",
      "==========\n",
      "Epoch: 422\n",
      "Loss:  0.0686\n",
      "==========\n",
      "Epoch: 423\n",
      "Loss:  0.0686\n",
      "==========\n",
      "Epoch: 424\n",
      "Loss:  0.0685\n",
      "==========\n",
      "Epoch: 425\n",
      "Loss:  0.0684\n",
      "==========\n",
      "Epoch: 426\n",
      "Loss:  0.0683\n",
      "==========\n",
      "Epoch: 427\n",
      "Loss:  0.0683\n",
      "==========\n",
      "Epoch: 428\n",
      "Loss:  0.0682\n",
      "==========\n",
      "Epoch: 429\n",
      "Loss:  0.0681\n",
      "==========\n",
      "Epoch: 430\n",
      "Loss:  0.0680\n",
      "==========\n",
      "Epoch: 431\n",
      "Loss:  0.0680\n",
      "==========\n",
      "Epoch: 432\n",
      "Loss:  0.0679\n",
      "==========\n",
      "Epoch: 433\n",
      "Loss:  0.0678\n",
      "==========\n",
      "Epoch: 434\n",
      "Loss:  0.0677\n",
      "==========\n",
      "Epoch: 435\n",
      "Loss:  0.0677\n",
      "==========\n",
      "Epoch: 436\n",
      "Loss:  0.0676\n",
      "==========\n",
      "Epoch: 437\n",
      "Loss:  0.0675\n",
      "==========\n",
      "Epoch: 438\n",
      "Loss:  0.0674\n",
      "==========\n",
      "Epoch: 439\n",
      "Loss:  0.0674\n",
      "==========\n",
      "Epoch: 440\n",
      "Loss:  0.0673\n",
      "==========\n",
      "Epoch: 441\n",
      "Loss:  0.0672\n",
      "==========\n",
      "Epoch: 442\n",
      "Loss:  0.0672\n",
      "==========\n",
      "Epoch: 443\n",
      "Loss:  0.0671\n",
      "==========\n",
      "Epoch: 444\n",
      "Loss:  0.0670\n",
      "==========\n",
      "Epoch: 445\n",
      "Loss:  0.0670\n",
      "==========\n",
      "Epoch: 446\n",
      "Loss:  0.0669\n",
      "==========\n",
      "Epoch: 447\n",
      "Loss:  0.0668\n",
      "==========\n",
      "Epoch: 448\n",
      "Loss:  0.0667\n",
      "==========\n",
      "Epoch: 449\n",
      "Loss:  0.0667\n",
      "==========\n",
      "Epoch: 450\n",
      "Loss:  0.0666\n",
      "==========\n",
      "Epoch: 451\n",
      "Loss:  0.0665\n",
      "==========\n",
      "Epoch: 452\n",
      "Loss:  0.0665\n",
      "==========\n",
      "Epoch: 453\n",
      "Loss:  0.0664\n",
      "==========\n",
      "Epoch: 454\n",
      "Loss:  0.0663\n",
      "==========\n",
      "Epoch: 455\n",
      "Loss:  0.0663\n",
      "==========\n",
      "Epoch: 456\n",
      "Loss:  0.0662\n",
      "==========\n",
      "Epoch: 457\n",
      "Loss:  0.0662\n",
      "==========\n",
      "Epoch: 458\n",
      "Loss:  0.0661\n",
      "==========\n",
      "Epoch: 459\n",
      "Loss:  0.0660\n",
      "==========\n",
      "Epoch: 460\n",
      "Loss:  0.0660\n",
      "==========\n",
      "Epoch: 461\n",
      "Loss:  0.0659\n",
      "==========\n",
      "Epoch: 462\n",
      "Loss:  0.0658\n",
      "==========\n",
      "Epoch: 463\n",
      "Loss:  0.0658\n",
      "==========\n",
      "Epoch: 464\n",
      "Loss:  0.0657\n",
      "==========\n",
      "Epoch: 465\n",
      "Loss:  0.0656\n",
      "==========\n",
      "Epoch: 466\n",
      "Loss:  0.0656\n",
      "==========\n",
      "Epoch: 467\n",
      "Loss:  0.0655\n",
      "==========\n",
      "Epoch: 468\n",
      "Loss:  0.0655\n",
      "==========\n",
      "Epoch: 469\n",
      "Loss:  0.0654\n",
      "==========\n",
      "Epoch: 470\n",
      "Loss:  0.0653\n",
      "==========\n",
      "Epoch: 471\n",
      "Loss:  0.0653\n",
      "==========\n",
      "Epoch: 472\n",
      "Loss:  0.0652\n",
      "==========\n",
      "Epoch: 473\n",
      "Loss:  0.0652\n",
      "==========\n",
      "Epoch: 474\n",
      "Loss:  0.0651\n",
      "==========\n",
      "Epoch: 475\n",
      "Loss:  0.0650\n",
      "==========\n",
      "Epoch: 476\n",
      "Loss:  0.0650\n",
      "==========\n",
      "Epoch: 477\n",
      "Loss:  0.0649\n",
      "==========\n",
      "Epoch: 478\n",
      "Loss:  0.0649\n",
      "==========\n",
      "Epoch: 479\n",
      "Loss:  0.0648\n",
      "==========\n",
      "Epoch: 480\n",
      "Loss:  0.0647\n",
      "==========\n",
      "Epoch: 481\n",
      "Loss:  0.0647\n",
      "==========\n",
      "Epoch: 482\n",
      "Loss:  0.0646\n",
      "==========\n",
      "Epoch: 483\n",
      "Loss:  0.0646\n",
      "==========\n",
      "Epoch: 484\n",
      "Loss:  0.0645\n",
      "==========\n",
      "Epoch: 485\n",
      "Loss:  0.0644\n",
      "==========\n",
      "Epoch: 486\n",
      "Loss:  0.0644\n",
      "==========\n",
      "Epoch: 487\n",
      "Loss:  0.0643\n",
      "==========\n",
      "Epoch: 488\n",
      "Loss:  0.0643\n",
      "==========\n",
      "Epoch: 489\n",
      "Loss:  0.0642\n",
      "==========\n",
      "Epoch: 490\n",
      "Loss:  0.0642\n",
      "==========\n",
      "Epoch: 491\n",
      "Loss:  0.0641\n",
      "==========\n",
      "Epoch: 492\n",
      "Loss:  0.0641\n",
      "==========\n",
      "Epoch: 493\n",
      "Loss:  0.0640\n",
      "==========\n",
      "Epoch: 494\n",
      "Loss:  0.0639\n",
      "==========\n",
      "Epoch: 495\n",
      "Loss:  0.0639\n",
      "==========\n",
      "Epoch: 496\n",
      "Loss:  0.0638\n",
      "==========\n",
      "Epoch: 497\n",
      "Loss:  0.0638\n",
      "==========\n",
      "Epoch: 498\n",
      "Loss:  0.0637\n",
      "==========\n",
      "Epoch: 499\n",
      "Loss:  0.0637\n",
      "==========\n",
      "Epoch: 500\n",
      "Loss:  0.0636\n",
      "==========\n",
      "Epoch: 501\n",
      "Loss:  0.0636\n",
      "==========\n",
      "Epoch: 502\n",
      "Loss:  0.0635\n",
      "==========\n",
      "Epoch: 503\n",
      "Loss:  0.0635\n",
      "==========\n",
      "Epoch: 504\n",
      "Loss:  0.0634\n",
      "==========\n",
      "Epoch: 505\n",
      "Loss:  0.0634\n",
      "==========\n",
      "Epoch: 506\n",
      "Loss:  0.0633\n",
      "==========\n",
      "Epoch: 507\n",
      "Loss:  0.0633\n",
      "==========\n",
      "Epoch: 508\n",
      "Loss:  0.0632\n",
      "==========\n",
      "Epoch: 509\n",
      "Loss:  0.0631\n",
      "==========\n",
      "Epoch: 510\n",
      "Loss:  0.0631\n",
      "==========\n",
      "Epoch: 511\n",
      "Loss:  0.0630\n",
      "==========\n",
      "Epoch: 512\n",
      "Loss:  0.0630\n",
      "==========\n",
      "Epoch: 513\n",
      "Loss:  0.0629\n",
      "==========\n",
      "Epoch: 514\n",
      "Loss:  0.0629\n",
      "==========\n",
      "Epoch: 515\n",
      "Loss:  0.0628\n",
      "==========\n",
      "Epoch: 516\n",
      "Loss:  0.0628\n",
      "==========\n",
      "Epoch: 517\n",
      "Loss:  0.0627\n",
      "==========\n",
      "Epoch: 518\n",
      "Loss:  0.0627\n",
      "==========\n",
      "Epoch: 519\n",
      "Loss:  0.0626\n",
      "==========\n",
      "Epoch: 520\n",
      "Loss:  0.0626\n",
      "==========\n",
      "Epoch: 521\n",
      "Loss:  0.0625\n",
      "==========\n",
      "Epoch: 522\n",
      "Loss:  0.0625\n",
      "==========\n",
      "Epoch: 523\n",
      "Loss:  0.0625\n",
      "==========\n",
      "Epoch: 524\n",
      "Loss:  0.0624\n",
      "==========\n",
      "Epoch: 525\n",
      "Loss:  0.0624\n",
      "==========\n",
      "Epoch: 526\n",
      "Loss:  0.0623\n",
      "==========\n",
      "Epoch: 527\n",
      "Loss:  0.0623\n",
      "==========\n",
      "Epoch: 528\n",
      "Loss:  0.0622\n",
      "==========\n",
      "Epoch: 529\n",
      "Loss:  0.0622\n",
      "==========\n",
      "Epoch: 530\n",
      "Loss:  0.0621\n",
      "==========\n",
      "Epoch: 531\n",
      "Loss:  0.0621\n",
      "==========\n",
      "Epoch: 532\n",
      "Loss:  0.0620\n",
      "==========\n",
      "Epoch: 533\n",
      "Loss:  0.0620\n",
      "==========\n",
      "Epoch: 534\n",
      "Loss:  0.0619\n",
      "==========\n",
      "Epoch: 535\n",
      "Loss:  0.0619\n",
      "==========\n",
      "Epoch: 536\n",
      "Loss:  0.0618\n",
      "==========\n",
      "Epoch: 537\n",
      "Loss:  0.0618\n",
      "==========\n",
      "Epoch: 538\n",
      "Loss:  0.0617\n",
      "==========\n",
      "Epoch: 539\n",
      "Loss:  0.0617\n",
      "==========\n",
      "Epoch: 540\n",
      "Loss:  0.0617\n",
      "==========\n",
      "Epoch: 541\n",
      "Loss:  0.0616\n",
      "==========\n",
      "Epoch: 542\n",
      "Loss:  0.0616\n",
      "==========\n",
      "Epoch: 543\n",
      "Loss:  0.0615\n",
      "==========\n",
      "Epoch: 544\n",
      "Loss:  0.0615\n",
      "==========\n",
      "Epoch: 545\n",
      "Loss:  0.0614\n",
      "==========\n",
      "Epoch: 546\n",
      "Loss:  0.0614\n",
      "==========\n",
      "Epoch: 547\n",
      "Loss:  0.0613\n",
      "==========\n",
      "Epoch: 548\n",
      "Loss:  0.0613\n",
      "==========\n",
      "Epoch: 549\n",
      "Loss:  0.0613\n",
      "==========\n",
      "Epoch: 550\n",
      "Loss:  0.0612\n",
      "==========\n",
      "Epoch: 551\n",
      "Loss:  0.0612\n",
      "==========\n",
      "Epoch: 552\n",
      "Loss:  0.0611\n",
      "==========\n",
      "Epoch: 553\n",
      "Loss:  0.0611\n",
      "==========\n",
      "Epoch: 554\n",
      "Loss:  0.0610\n",
      "==========\n",
      "Epoch: 555\n",
      "Loss:  0.0610\n",
      "==========\n",
      "Epoch: 556\n",
      "Loss:  0.0610\n",
      "==========\n",
      "Epoch: 557\n",
      "Loss:  0.0609\n",
      "==========\n",
      "Epoch: 558\n",
      "Loss:  0.0609\n",
      "==========\n",
      "Epoch: 559\n",
      "Loss:  0.0608\n",
      "==========\n",
      "Epoch: 560\n",
      "Loss:  0.0608\n",
      "==========\n",
      "Epoch: 561\n",
      "Loss:  0.0607\n",
      "==========\n",
      "Epoch: 562\n",
      "Loss:  0.0607\n",
      "==========\n",
      "Epoch: 563\n",
      "Loss:  0.0607\n",
      "==========\n",
      "Epoch: 564\n",
      "Loss:  0.0606\n",
      "==========\n",
      "Epoch: 565\n",
      "Loss:  0.0606\n",
      "==========\n",
      "Epoch: 566\n",
      "Loss:  0.0605\n",
      "==========\n",
      "Epoch: 567\n",
      "Loss:  0.0605\n",
      "==========\n",
      "Epoch: 568\n",
      "Loss:  0.0605\n",
      "==========\n",
      "Epoch: 569\n",
      "Loss:  0.0604\n",
      "==========\n",
      "Epoch: 570\n",
      "Loss:  0.0604\n",
      "==========\n",
      "Epoch: 571\n",
      "Loss:  0.0603\n",
      "==========\n",
      "Epoch: 572\n",
      "Loss:  0.0603\n",
      "==========\n",
      "Epoch: 573\n",
      "Loss:  0.0603\n",
      "==========\n",
      "Epoch: 574\n",
      "Loss:  0.0602\n",
      "==========\n",
      "Epoch: 575\n",
      "Loss:  0.0602\n",
      "==========\n",
      "Epoch: 576\n",
      "Loss:  0.0601\n",
      "==========\n",
      "Epoch: 577\n",
      "Loss:  0.0601\n",
      "==========\n",
      "Epoch: 578\n",
      "Loss:  0.0601\n",
      "==========\n",
      "Epoch: 579\n",
      "Loss:  0.0600\n",
      "==========\n",
      "Epoch: 580\n",
      "Loss:  0.0600\n",
      "==========\n",
      "Epoch: 581\n",
      "Loss:  0.0599\n",
      "==========\n",
      "Epoch: 582\n",
      "Loss:  0.0599\n",
      "==========\n",
      "Epoch: 583\n",
      "Loss:  0.0599\n",
      "==========\n",
      "Epoch: 584\n",
      "Loss:  0.0598\n",
      "==========\n",
      "Epoch: 585\n",
      "Loss:  0.0598\n",
      "==========\n",
      "Epoch: 586\n",
      "Loss:  0.0598\n",
      "==========\n",
      "Epoch: 587\n",
      "Loss:  0.0597\n",
      "==========\n",
      "Epoch: 588\n",
      "Loss:  0.0597\n",
      "==========\n",
      "Epoch: 589\n",
      "Loss:  0.0596\n",
      "==========\n",
      "Epoch: 590\n",
      "Loss:  0.0596\n",
      "==========\n",
      "Epoch: 591\n",
      "Loss:  0.0596\n",
      "==========\n",
      "Epoch: 592\n",
      "Loss:  0.0595\n",
      "==========\n",
      "Epoch: 593\n",
      "Loss:  0.0595\n",
      "==========\n",
      "Epoch: 594\n",
      "Loss:  0.0595\n",
      "==========\n",
      "Epoch: 595\n",
      "Loss:  0.0594\n",
      "==========\n",
      "Epoch: 596\n",
      "Loss:  0.0594\n",
      "==========\n",
      "Epoch: 597\n",
      "Loss:  0.0594\n",
      "==========\n",
      "Epoch: 598\n",
      "Loss:  0.0593\n",
      "==========\n",
      "Epoch: 599\n",
      "Loss:  0.0593\n",
      "==========\n",
      "Epoch: 600\n",
      "Loss:  0.0592\n",
      "==========\n",
      "Epoch: 601\n",
      "Loss:  0.0592\n",
      "==========\n",
      "Epoch: 602\n",
      "Loss:  0.0592\n",
      "==========\n",
      "Epoch: 603\n",
      "Loss:  0.0591\n",
      "==========\n",
      "Epoch: 604\n",
      "Loss:  0.0591\n",
      "==========\n",
      "Epoch: 605\n",
      "Loss:  0.0591\n",
      "==========\n",
      "Epoch: 606\n",
      "Loss:  0.0590\n",
      "==========\n",
      "Epoch: 607\n",
      "Loss:  0.0590\n",
      "==========\n",
      "Epoch: 608\n",
      "Loss:  0.0590\n",
      "==========\n",
      "Epoch: 609\n",
      "Loss:  0.0589\n",
      "==========\n",
      "Epoch: 610\n",
      "Loss:  0.0589\n",
      "==========\n",
      "Epoch: 611\n",
      "Loss:  0.0589\n",
      "==========\n",
      "Epoch: 612\n",
      "Loss:  0.0588\n",
      "==========\n",
      "Epoch: 613\n",
      "Loss:  0.0588\n",
      "==========\n",
      "Epoch: 614\n",
      "Loss:  0.0588\n",
      "==========\n",
      "Epoch: 615\n",
      "Loss:  0.0587\n",
      "==========\n",
      "Epoch: 616\n",
      "Loss:  0.0587\n",
      "==========\n",
      "Epoch: 617\n",
      "Loss:  0.0587\n",
      "==========\n",
      "Epoch: 618\n",
      "Loss:  0.0586\n",
      "==========\n",
      "Epoch: 619\n",
      "Loss:  0.0586\n",
      "==========\n",
      "Epoch: 620\n",
      "Loss:  0.0586\n",
      "==========\n",
      "Epoch: 621\n",
      "Loss:  0.0585\n",
      "==========\n",
      "Epoch: 622\n",
      "Loss:  0.0585\n",
      "==========\n",
      "Epoch: 623\n",
      "Loss:  0.0585\n",
      "==========\n",
      "Epoch: 624\n",
      "Loss:  0.0584\n",
      "==========\n",
      "Epoch: 625\n",
      "Loss:  0.0584\n",
      "==========\n",
      "Epoch: 626\n",
      "Loss:  0.0584\n",
      "==========\n",
      "Epoch: 627\n",
      "Loss:  0.0583\n",
      "==========\n",
      "Epoch: 628\n",
      "Loss:  0.0583\n",
      "==========\n",
      "Epoch: 629\n",
      "Loss:  0.0583\n",
      "==========\n",
      "Epoch: 630\n",
      "Loss:  0.0582\n",
      "==========\n",
      "Epoch: 631\n",
      "Loss:  0.0582\n",
      "==========\n",
      "Epoch: 632\n",
      "Loss:  0.0582\n",
      "==========\n",
      "Epoch: 633\n",
      "Loss:  0.0581\n",
      "==========\n",
      "Epoch: 634\n",
      "Loss:  0.0581\n",
      "==========\n",
      "Epoch: 635\n",
      "Loss:  0.0581\n",
      "==========\n",
      "Epoch: 636\n",
      "Loss:  0.0580\n",
      "==========\n",
      "Epoch: 637\n",
      "Loss:  0.0580\n",
      "==========\n",
      "Epoch: 638\n",
      "Loss:  0.0580\n",
      "==========\n",
      "Epoch: 639\n",
      "Loss:  0.0580\n",
      "==========\n",
      "Epoch: 640\n",
      "Loss:  0.0579\n",
      "==========\n",
      "Epoch: 641\n",
      "Loss:  0.0579\n",
      "==========\n",
      "Epoch: 642\n",
      "Loss:  0.0579\n",
      "==========\n",
      "Epoch: 643\n",
      "Loss:  0.0578\n",
      "==========\n",
      "Epoch: 644\n",
      "Loss:  0.0578\n",
      "==========\n",
      "Epoch: 645\n",
      "Loss:  0.0578\n",
      "==========\n",
      "Epoch: 646\n",
      "Loss:  0.0577\n",
      "==========\n",
      "Epoch: 647\n",
      "Loss:  0.0577\n",
      "==========\n",
      "Epoch: 648\n",
      "Loss:  0.0577\n",
      "==========\n",
      "Epoch: 649\n",
      "Loss:  0.0576\n",
      "==========\n",
      "Epoch: 650\n",
      "Loss:  0.0576\n",
      "==========\n",
      "Epoch: 651\n",
      "Loss:  0.0576\n",
      "==========\n",
      "Epoch: 652\n",
      "Loss:  0.0576\n",
      "==========\n",
      "Epoch: 653\n",
      "Loss:  0.0575\n",
      "==========\n",
      "Epoch: 654\n",
      "Loss:  0.0575\n",
      "==========\n",
      "Epoch: 655\n",
      "Loss:  0.0575\n",
      "==========\n",
      "Epoch: 656\n",
      "Loss:  0.0574\n",
      "==========\n",
      "Epoch: 657\n",
      "Loss:  0.0574\n",
      "==========\n",
      "Epoch: 658\n",
      "Loss:  0.0574\n",
      "==========\n",
      "Epoch: 659\n",
      "Loss:  0.0574\n",
      "==========\n",
      "Epoch: 660\n",
      "Loss:  0.0573\n",
      "==========\n",
      "Epoch: 661\n",
      "Loss:  0.0573\n",
      "==========\n",
      "Epoch: 662\n",
      "Loss:  0.0573\n",
      "==========\n",
      "Epoch: 663\n",
      "Loss:  0.0572\n",
      "==========\n",
      "Epoch: 664\n",
      "Loss:  0.0572\n",
      "==========\n",
      "Epoch: 665\n",
      "Loss:  0.0572\n",
      "==========\n",
      "Epoch: 666\n",
      "Loss:  0.0572\n",
      "==========\n",
      "Epoch: 667\n",
      "Loss:  0.0571\n",
      "==========\n",
      "Epoch: 668\n",
      "Loss:  0.0571\n",
      "==========\n",
      "Epoch: 669\n",
      "Loss:  0.0571\n",
      "==========\n",
      "Epoch: 670\n",
      "Loss:  0.0570\n",
      "==========\n",
      "Epoch: 671\n",
      "Loss:  0.0570\n",
      "==========\n",
      "Epoch: 672\n",
      "Loss:  0.0570\n",
      "==========\n",
      "Epoch: 673\n",
      "Loss:  0.0570\n",
      "==========\n",
      "Epoch: 674\n",
      "Loss:  0.0569\n",
      "==========\n",
      "Epoch: 675\n",
      "Loss:  0.0569\n",
      "==========\n",
      "Epoch: 676\n",
      "Loss:  0.0569\n",
      "==========\n",
      "Epoch: 677\n",
      "Loss:  0.0568\n",
      "==========\n",
      "Epoch: 678\n",
      "Loss:  0.0568\n",
      "==========\n",
      "Epoch: 679\n",
      "Loss:  0.0568\n",
      "==========\n",
      "Epoch: 680\n",
      "Loss:  0.0568\n",
      "==========\n",
      "Epoch: 681\n",
      "Loss:  0.0567\n",
      "==========\n",
      "Epoch: 682\n",
      "Loss:  0.0567\n",
      "==========\n",
      "Epoch: 683\n",
      "Loss:  0.0567\n",
      "==========\n",
      "Epoch: 684\n",
      "Loss:  0.0567\n",
      "==========\n",
      "Epoch: 685\n",
      "Loss:  0.0566\n",
      "==========\n",
      "Epoch: 686\n",
      "Loss:  0.0566\n",
      "==========\n",
      "Epoch: 687\n",
      "Loss:  0.0566\n",
      "==========\n",
      "Epoch: 688\n",
      "Loss:  0.0566\n",
      "==========\n",
      "Epoch: 689\n",
      "Loss:  0.0565\n",
      "==========\n",
      "Epoch: 690\n",
      "Loss:  0.0565\n",
      "==========\n",
      "Epoch: 691\n",
      "Loss:  0.0565\n",
      "==========\n",
      "Epoch: 692\n",
      "Loss:  0.0564\n",
      "==========\n",
      "Epoch: 693\n",
      "Loss:  0.0564\n",
      "==========\n",
      "Epoch: 694\n",
      "Loss:  0.0564\n",
      "==========\n",
      "Epoch: 695\n",
      "Loss:  0.0564\n",
      "==========\n",
      "Epoch: 696\n",
      "Loss:  0.0563\n",
      "==========\n",
      "Epoch: 697\n",
      "Loss:  0.0563\n",
      "==========\n",
      "Epoch: 698\n",
      "Loss:  0.0563\n",
      "==========\n",
      "Epoch: 699\n",
      "Loss:  0.0563\n",
      "==========\n",
      "Epoch: 700\n",
      "Loss:  0.0562\n",
      "==========\n",
      "Epoch: 701\n",
      "Loss:  0.0562\n",
      "==========\n",
      "Epoch: 702\n",
      "Loss:  0.0562\n",
      "==========\n",
      "Epoch: 703\n",
      "Loss:  0.0562\n",
      "==========\n",
      "Epoch: 704\n",
      "Loss:  0.0561\n",
      "==========\n",
      "Epoch: 705\n",
      "Loss:  0.0561\n",
      "==========\n",
      "Epoch: 706\n",
      "Loss:  0.0561\n",
      "==========\n",
      "Epoch: 707\n",
      "Loss:  0.0561\n",
      "==========\n",
      "Epoch: 708\n",
      "Loss:  0.0560\n",
      "==========\n",
      "Epoch: 709\n",
      "Loss:  0.0560\n",
      "==========\n",
      "Epoch: 710\n",
      "Loss:  0.0560\n",
      "==========\n",
      "Epoch: 711\n",
      "Loss:  0.0560\n",
      "==========\n",
      "Epoch: 712\n",
      "Loss:  0.0559\n",
      "==========\n",
      "Epoch: 713\n",
      "Loss:  0.0559\n",
      "==========\n",
      "Epoch: 714\n",
      "Loss:  0.0559\n",
      "==========\n",
      "Epoch: 715\n",
      "Loss:  0.0559\n",
      "==========\n",
      "Epoch: 716\n",
      "Loss:  0.0558\n",
      "==========\n",
      "Epoch: 717\n",
      "Loss:  0.0558\n",
      "==========\n",
      "Epoch: 718\n",
      "Loss:  0.0558\n",
      "==========\n",
      "Epoch: 719\n",
      "Loss:  0.0558\n",
      "==========\n",
      "Epoch: 720\n",
      "Loss:  0.0558\n",
      "==========\n",
      "Epoch: 721\n",
      "Loss:  0.0557\n",
      "==========\n",
      "Epoch: 722\n",
      "Loss:  0.0557\n",
      "==========\n",
      "Epoch: 723\n",
      "Loss:  0.0557\n",
      "==========\n",
      "Epoch: 724\n",
      "Loss:  0.0557\n",
      "==========\n",
      "Epoch: 725\n",
      "Loss:  0.0556\n",
      "==========\n",
      "Epoch: 726\n",
      "Loss:  0.0556\n",
      "==========\n",
      "Epoch: 727\n",
      "Loss:  0.0556\n",
      "==========\n",
      "Epoch: 728\n",
      "Loss:  0.0556\n",
      "==========\n",
      "Epoch: 729\n",
      "Loss:  0.0555\n",
      "==========\n",
      "Epoch: 730\n",
      "Loss:  0.0555\n",
      "==========\n",
      "Epoch: 731\n",
      "Loss:  0.0555\n",
      "==========\n",
      "Epoch: 732\n",
      "Loss:  0.0555\n",
      "==========\n",
      "Epoch: 733\n",
      "Loss:  0.0554\n",
      "==========\n",
      "Epoch: 734\n",
      "Loss:  0.0554\n",
      "==========\n",
      "Epoch: 735\n",
      "Loss:  0.0554\n",
      "==========\n",
      "Epoch: 736\n",
      "Loss:  0.0554\n",
      "==========\n",
      "Epoch: 737\n",
      "Loss:  0.0554\n",
      "==========\n",
      "Epoch: 738\n",
      "Loss:  0.0553\n",
      "==========\n",
      "Epoch: 739\n",
      "Loss:  0.0553\n",
      "==========\n",
      "Epoch: 740\n",
      "Loss:  0.0553\n",
      "==========\n",
      "Epoch: 741\n",
      "Loss:  0.0553\n",
      "==========\n",
      "Epoch: 742\n",
      "Loss:  0.0552\n",
      "==========\n",
      "Epoch: 743\n",
      "Loss:  0.0552\n",
      "==========\n",
      "Epoch: 744\n",
      "Loss:  0.0552\n",
      "==========\n",
      "Epoch: 745\n",
      "Loss:  0.0552\n",
      "==========\n",
      "Epoch: 746\n",
      "Loss:  0.0552\n",
      "==========\n",
      "Epoch: 747\n",
      "Loss:  0.0551\n",
      "==========\n",
      "Epoch: 748\n",
      "Loss:  0.0551\n",
      "==========\n",
      "Epoch: 749\n",
      "Loss:  0.0551\n",
      "==========\n",
      "Epoch: 750\n",
      "Loss:  0.0551\n",
      "==========\n",
      "Epoch: 751\n",
      "Loss:  0.0550\n",
      "==========\n",
      "Epoch: 752\n",
      "Loss:  0.0550\n",
      "==========\n",
      "Epoch: 753\n",
      "Loss:  0.0550\n",
      "==========\n",
      "Epoch: 754\n",
      "Loss:  0.0550\n",
      "==========\n",
      "Epoch: 755\n",
      "Loss:  0.0550\n",
      "==========\n",
      "Epoch: 756\n",
      "Loss:  0.0549\n",
      "==========\n",
      "Epoch: 757\n",
      "Loss:  0.0549\n",
      "==========\n",
      "Epoch: 758\n",
      "Loss:  0.0549\n",
      "==========\n",
      "Epoch: 759\n",
      "Loss:  0.0549\n",
      "==========\n",
      "Epoch: 760\n",
      "Loss:  0.0549\n",
      "==========\n",
      "Epoch: 761\n",
      "Loss:  0.0548\n",
      "==========\n",
      "Epoch: 762\n",
      "Loss:  0.0548\n",
      "==========\n",
      "Epoch: 763\n",
      "Loss:  0.0548\n",
      "==========\n",
      "Epoch: 764\n",
      "Loss:  0.0548\n",
      "==========\n",
      "Epoch: 765\n",
      "Loss:  0.0547\n",
      "==========\n",
      "Epoch: 766\n",
      "Loss:  0.0547\n",
      "==========\n",
      "Epoch: 767\n",
      "Loss:  0.0547\n",
      "==========\n",
      "Epoch: 768\n",
      "Loss:  0.0547\n",
      "==========\n",
      "Epoch: 769\n",
      "Loss:  0.0547\n",
      "==========\n",
      "Epoch: 770\n",
      "Loss:  0.0546\n",
      "==========\n",
      "Epoch: 771\n",
      "Loss:  0.0546\n",
      "==========\n",
      "Epoch: 772\n",
      "Loss:  0.0546\n",
      "==========\n",
      "Epoch: 773\n",
      "Loss:  0.0546\n",
      "==========\n",
      "Epoch: 774\n",
      "Loss:  0.0546\n",
      "==========\n",
      "Epoch: 775\n",
      "Loss:  0.0545\n",
      "==========\n",
      "Epoch: 776\n",
      "Loss:  0.0545\n",
      "==========\n",
      "Epoch: 777\n",
      "Loss:  0.0545\n",
      "==========\n",
      "Epoch: 778\n",
      "Loss:  0.0545\n",
      "==========\n",
      "Epoch: 779\n",
      "Loss:  0.0545\n",
      "==========\n",
      "Epoch: 780\n",
      "Loss:  0.0544\n",
      "==========\n",
      "Epoch: 781\n",
      "Loss:  0.0544\n",
      "==========\n",
      "Epoch: 782\n",
      "Loss:  0.0544\n",
      "==========\n",
      "Epoch: 783\n",
      "Loss:  0.0544\n",
      "==========\n",
      "Epoch: 784\n",
      "Loss:  0.0544\n",
      "==========\n",
      "Epoch: 785\n",
      "Loss:  0.0543\n",
      "==========\n",
      "Epoch: 786\n",
      "Loss:  0.0543\n",
      "==========\n",
      "Epoch: 787\n",
      "Loss:  0.0543\n",
      "==========\n",
      "Epoch: 788\n",
      "Loss:  0.0543\n",
      "==========\n",
      "Epoch: 789\n",
      "Loss:  0.0543\n",
      "==========\n",
      "Epoch: 790\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 791\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 792\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 793\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 794\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 795\n",
      "Loss:  0.0542\n",
      "==========\n",
      "Epoch: 796\n",
      "Loss:  0.0541\n",
      "==========\n",
      "Epoch: 797\n",
      "Loss:  0.0541\n",
      "==========\n",
      "Epoch: 798\n",
      "Loss:  0.0541\n",
      "==========\n",
      "Epoch: 799\n",
      "Loss:  0.0541\n",
      "==========\n",
      "Epoch: 800\n",
      "Loss:  0.0541\n",
      "==========\n",
      "Epoch: 801\n",
      "Loss:  0.0540\n",
      "==========\n",
      "Epoch: 802\n",
      "Loss:  0.0540\n",
      "==========\n",
      "Epoch: 803\n",
      "Loss:  0.0540\n",
      "==========\n",
      "Epoch: 804\n",
      "Loss:  0.0540\n",
      "==========\n",
      "Epoch: 805\n",
      "Loss:  0.0540\n",
      "==========\n",
      "Epoch: 806\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 807\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 808\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 809\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 810\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 811\n",
      "Loss:  0.0539\n",
      "==========\n",
      "Epoch: 812\n",
      "Loss:  0.0538\n",
      "==========\n",
      "Epoch: 813\n",
      "Loss:  0.0538\n",
      "==========\n",
      "Epoch: 814\n",
      "Loss:  0.0538\n",
      "==========\n",
      "Epoch: 815\n",
      "Loss:  0.0538\n",
      "==========\n",
      "Epoch: 816\n",
      "Loss:  0.0538\n",
      "==========\n",
      "Epoch: 817\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 818\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 819\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 820\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 821\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 822\n",
      "Loss:  0.0537\n",
      "==========\n",
      "Epoch: 823\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 824\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 825\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 826\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 827\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 828\n",
      "Loss:  0.0536\n",
      "==========\n",
      "Epoch: 829\n",
      "Loss:  0.0535\n",
      "==========\n",
      "Epoch: 830\n",
      "Loss:  0.0535\n",
      "==========\n",
      "Epoch: 831\n",
      "Loss:  0.0535\n",
      "==========\n",
      "Epoch: 832\n",
      "Loss:  0.0535\n",
      "==========\n",
      "Epoch: 833\n",
      "Loss:  0.0535\n",
      "==========\n",
      "Epoch: 834\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 835\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 836\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 837\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 838\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 839\n",
      "Loss:  0.0534\n",
      "==========\n",
      "Epoch: 840\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 841\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 842\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 843\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 844\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 845\n",
      "Loss:  0.0533\n",
      "==========\n",
      "Epoch: 846\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 847\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 848\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 849\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 850\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 851\n",
      "Loss:  0.0532\n",
      "==========\n",
      "Epoch: 852\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 853\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 854\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 855\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 856\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 857\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 858\n",
      "Loss:  0.0531\n",
      "==========\n",
      "Epoch: 859\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 860\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 861\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 862\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 863\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 864\n",
      "Loss:  0.0530\n",
      "==========\n",
      "Epoch: 865\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 866\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 867\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 868\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 869\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 870\n",
      "Loss:  0.0529\n",
      "==========\n",
      "Epoch: 871\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 872\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 873\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 874\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 875\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 876\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 877\n",
      "Loss:  0.0528\n",
      "==========\n",
      "Epoch: 878\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 879\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 880\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 881\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 882\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 883\n",
      "Loss:  0.0527\n",
      "==========\n",
      "Epoch: 884\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 885\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 886\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 887\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 888\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 889\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 890\n",
      "Loss:  0.0526\n",
      "==========\n",
      "Epoch: 891\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 892\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 893\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 894\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 895\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 896\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 897\n",
      "Loss:  0.0525\n",
      "==========\n",
      "Epoch: 898\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 899\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 900\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 901\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 902\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 903\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 904\n",
      "Loss:  0.0524\n",
      "==========\n",
      "Epoch: 905\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 906\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 907\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 908\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 909\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 910\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 911\n",
      "Loss:  0.0523\n",
      "==========\n",
      "Epoch: 912\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 913\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 914\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 915\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 916\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 917\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 918\n",
      "Loss:  0.0522\n",
      "==========\n",
      "Epoch: 919\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 920\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 921\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 922\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 923\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 924\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 925\n",
      "Loss:  0.0521\n",
      "==========\n",
      "Epoch: 926\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 927\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 928\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 929\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 930\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 931\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 932\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 933\n",
      "Loss:  0.0520\n",
      "==========\n",
      "Epoch: 934\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 935\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 936\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 937\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 938\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 939\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 940\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 941\n",
      "Loss:  0.0519\n",
      "==========\n",
      "Epoch: 942\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 943\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 944\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 945\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 946\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 947\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 948\n",
      "Loss:  0.0518\n",
      "==========\n",
      "Epoch: 949\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 950\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 951\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 952\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 953\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 954\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 955\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 956\n",
      "Loss:  0.0517\n",
      "==========\n",
      "Epoch: 957\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 958\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 959\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 960\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 961\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 962\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 963\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 964\n",
      "Loss:  0.0516\n",
      "==========\n",
      "Epoch: 965\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 966\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 967\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 968\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 969\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 970\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 971\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 972\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 973\n",
      "Loss:  0.0515\n",
      "==========\n",
      "Epoch: 974\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 975\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 976\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 977\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 978\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 979\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 980\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 981\n",
      "Loss:  0.0514\n",
      "==========\n",
      "Epoch: 982\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 983\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 984\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 985\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 986\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 987\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 988\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 989\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 990\n",
      "Loss:  0.0513\n",
      "==========\n",
      "Epoch: 991\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 992\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 993\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 994\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 995\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 996\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 997\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 998\n",
      "Loss:  0.0512\n",
      "==========\n",
      "Epoch: 999\n",
      "Loss:  0.0511\n",
      "==========\n",
      "Epoch: 1000\n",
      "Loss:  0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = nn.train(X_train, y_train, epochs=1000, learning_rate=0.001, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "acc = accuracy(y_pred, y_test)\n",
    "cm = confusion_matrix(y_pred, y_test)\n",
    "precision, recall = precision_recall(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0000\n",
      "Confusion Matrix:\n",
      " [[10.  0.  0.]\n",
      " [ 0.  9.  0.]\n",
      " [ 0.  0. 11.]]\n",
      "Precision: [1. 1. 1.]\n",
      "Recall: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARi1JREFUeJzt3Xl4VNXBx/HfnSwzSUgmBCQJAgGVHdmFBASlSASESq2FqkRosZRWLRR9qyjuC9qntoAoiqIpVVlsQGkFFSqLlgiyJMW6QQUSQyKyJJMAWee+fyQZMiRASMLchPl+nuc+yT1z7p1zr7zN7z3n3HMN0zRNAQAA+BGb1Q0AAADwNQIQAADwOwQgAADgdwhAAADA7xCAAACA3yEAAQAAv0MAAgAAfocABAAA/A4BCAAA+B0CEIAzMgyjVtvGjRvr9T2PPvqoDMOo07EbN25skDbU57v//ve/+/y7AdRPoNUNANB4paameu0/8cQT2rBhgz766COv8m7dutXre+644w6NHDmyTsf27dtXqamp9W4DAP9CAAJwRvHx8V77l1xyiWw2W7Xy0504cUKhoaG1/p42bdqoTZs2dWpjRETEOdsDAKdjCAxAvVx77bXq0aOHNm/erEGDBik0NFS//OUvJUnLly9XYmKiYmNjFRISoq5du+r+++/X8ePHvc5R0xBY+/btNWbMGL3//vvq27evQkJC1KVLF7322mte9WoaAps8ebKaNWumvXv3avTo0WrWrJnatm2re+65R0VFRV7Hf/fdd7r55psVHh6uyMhI3Xbbbfrss89kGIaSk5Mb5B59/vnnuvHGG9W8eXM5HA717t1bf/3rX73quN1uPfnkk+rcubNCQkIUGRmpnj17at68eZ46P/zwg6ZOnaq2bdvKbrfrkksu0eDBg7V+/foGaSfgT+gBAlBv2dnZmjhxov7whz/o6aefls1W/v9b7dmzR6NHj9aMGTMUFhamr776Ss8++6y2bdtWbRitJunp6brnnnt0//33Kzo6Wq+++qqmTJmiK664QkOHDj3rsSUlJfrxj3+sKVOm6J577tHmzZv1xBNPyOl06uGHH5YkHT9+XMOGDdPRo0f17LPP6oorrtD777+vCRMm1P+mVPj66681aNAgtWrVSvPnz1eLFi30xhtvaPLkyfr+++/1hz/8QZL0xz/+UY8++qhmz56toUOHqqSkRF999ZVyc3M950pKStLOnTv11FNPqVOnTsrNzdXOnTt15MiRBmsv4DdMAKilSZMmmWFhYV5l11xzjSnJ/Ne//nXWY91ut1lSUmJu2rTJlGSmp6d7PnvkkUfM0//nKC4uznQ4HOaBAwc8ZSdPnjSjoqLMX//6156yDRs2mJLMDRs2eLVTkrlixQqvc44ePdrs3LmzZ/+FF14wJZlr1671qvfrX//alGS+/vrrZ72myu9+++23z1jn5z//uWm3282MjAyv8lGjRpmhoaFmbm6uaZqmOWbMGLN3795n/b5mzZqZM2bMOGsdALXDEBiAemvevLl+9KMfVSv/9ttvdeuttyomJkYBAQEKCgrSNddcI0n68ssvz3ne3r17q127dp59h8OhTp066cCBA+c81jAMjR071qusZ8+eXsdu2rRJ4eHh1SZg33LLLec8f2199NFHGj58uNq2betVPnnyZJ04ccIz0XzAgAFKT0/Xb3/7W33wwQdyuVzVzjVgwAAlJyfrySef1KeffqqSkpIGayfgbwhAAOotNja2WllBQYGGDBmirVu36sknn9TGjRv12WefaeXKlZKkkydPnvO8LVq0qFZmt9trdWxoaKgcDke1YwsLCz37R44cUXR0dLVjayqrqyNHjtR4f1q3bu35XJJmzZqlP/3pT/r00081atQotWjRQsOHD9f27ds9xyxfvlyTJk3Sq6++qoSEBEVFRen2229XTk5Og7UX8BcEIAD1VtMaPh999JEOHjyo1157TXfccYeGDh2q/v37Kzw83IIW1qxFixb6/vvvq5U3ZKBo0aKFsrOzq5UfPHhQktSyZUtJUmBgoGbOnKmdO3fq6NGjWrp0qTIzM3X99dfrxIkTnrpz587V/v37deDAAc2ZM0crV67U5MmTG6y9gL8gAAG4ICpDkd1u9yp/+eWXrWhOja655hrl5+dr7dq1XuXLli1rsO8YPny4JwxWtWTJEoWGhtb4CH9kZKRuvvlm3XnnnTp69Kj2799frU67du101113acSIEdq5c2eDtRfwFzwFBuCCGDRokJo3b65p06bpkUceUVBQkN58802lp6db3TSPSZMm6S9/+YsmTpyoJ598UldccYXWrl2rDz74QJI8T7Ody6efflpj+TXXXKNHHnlE//znPzVs2DA9/PDDioqK0ptvvqn33ntPf/zjH+V0OiVJY8eOVY8ePdS/f39dcsklOnDggObOnau4uDh17NhReXl5GjZsmG699VZ16dJF4eHh+uyzz/T+++/rpptuapgbAvgRAhCAC6JFixZ67733dM8992jixIkKCwvTjTfeqOXLl6tv375WN0+SFBYWpo8++kgzZszQH/7wBxmGocTERL344osaPXq0IiMja3We5557rsbyDRs26Nprr9WWLVv0wAMP6M4779TJkyfVtWtXvf76615DV8OGDVNKSopeffVVuVwuxcTEaMSIEXrooYcUFBQkh8OhgQMH6m9/+5v279+vkpIStWvXTvfdd5/nUXoAtWeYpmla3QgAaEyefvppzZ49WxkZGXVeoRpA40YPEAC/tmDBAklSly5dVFJSoo8++kjz58/XxIkTCT/ARYwABMCvhYaG6i9/+Yv279+voqIiz7DS7NmzrW4agAuIITAAAOB3eAweAAD4HQIQAADwOwQgAADgd5gEXQO3262DBw8qPDy8xiX+AQBA42OapvLz89W6detzLmRKAKrBwYMHq725GQAANA2ZmZnnXMaCAFSDypc1ZmZmKiIiwuLWAACA2nC5XGrbtm2tXrpMAKpB5bBXREQEAQgAgCamNtNXmAQNAAD8DgEIAAD4HQIQAADwOwQgAADgdwhAAADA7xCAAACA3yEAAQAAv0MAAgAAfocABAAA/A4BCAAA+B0CEAAA8DsEIAAA4HcsDUCbN2/W2LFj1bp1axmGoXfeeees9SdPnizDMKpt3bt399RJTk6usU5hYeEFvppzKyot03fHTignz/q2AADgzywNQMePH1evXr20YMGCWtWfN2+esrOzPVtmZqaioqL0s5/9zKteRESEV73s7Gw5HI4LcQnn5fMsl65+doPGv5xqdVMAAPBrgVZ++ahRozRq1Kha13c6nXI6nZ79d955R8eOHdMvfvELr3qGYSgmJqbB2tnQTJlWNwEAAL/WpOcALV68WNddd53i4uK8ygsKChQXF6c2bdpozJgx2rVrl0Ut9GYY5T9N8g8AAJaytAeoPrKzs7V27Vq99dZbXuVdunRRcnKyrrzySrlcLs2bN0+DBw9Wenq6OnbsWOO5ioqKVFRU5Nl3uVwXpM0V+YcABACAxZpsD1BycrIiIyM1btw4r/L4+HhNnDhRvXr10pAhQ7RixQp16tRJzz///BnPNWfOHM/wmtPpVNu2bS9Im43KLiAAAGCpJhmATNPUa6+9pqSkJAUHB5+1rs1m01VXXaU9e/acsc6sWbOUl5fn2TIzMxu6yZJO9QABAABrNckhsE2bNmnv3r2aMmXKOeuapqm0tDRdeeWVZ6xjt9tlt9sbsok1OjUHiDEwAACsZGkAKigo0N69ez37+/btU1pamqKiotSuXTvNmjVLWVlZWrJkiddxixcv1sCBA9WjR49q53zssccUHx+vjh07yuVyaf78+UpLS9MLL7xwwa+ntog/AABYy9IAtH37dg0bNsyzP3PmTEnSpEmTlJycrOzsbGVkZHgdk5eXp5SUFM2bN6/Gc+bm5mrq1KnKycmR0+lUnz59tHnzZg0YMODCXUgtGRWDYHQAAQBgLcNkPKYal8slp9OpvLw8RURENNh5P8/K05jnP1F0hF1bH7iuwc4LAADO7+93k5wE3dQROQEAsBYById4Ch4AgMaBAGQBOoAAALAWAciHmAQNAEDjQADyoVNDYCQgAACsRADyIV6GCgBA40AA8iGDl2EAANAoEIB8yNMDZG0zAADwewQgC7D2JAAA1iIA+VDlABjxBwAAaxGAfIhJ0AAANA4EIJ+qXAeIBAQAgJUIQD7EqzAAAGgcCEAWoP8HAABrEYB8iIWgAQBoHAhAPmRUjIGRfwAAsBYByIc8j8EzCRoAAEsRgHyISdAAADQOBCAfqnwXGP0/AABYiwBkAUbAAACwFgHIh069DJUEBACAlQhAFqAHCAAAaxGAfIhJ0AAANA4EIB9iHSAAABoHApAPsRI0AACNAwHIAkyCBgDAWgQgH/I8BUb+AQDAUgQgH2IhRAAAGgcCkA/xFBgAAI0DAciHeBkqAACNAwHIAsQfAACsRQDyJSZBAwDQKBCAfMgQk4AAAGgMCEA+xCRoAAAaBwKQD1XNP0yEBgDAOgQgHzKqdAGRfwAAsA4ByCLkHwAArEMA8iGGwAAAaBwsDUCbN2/W2LFj1bp1axmGoXfeeees9Tdu3CjDMKptX331lVe9lJQUdevWTXa7Xd26ddOqVasu4FXUHpOgAQBoHCwNQMePH1evXr20YMGC8zru66+/VnZ2tmfr2LGj57PU1FRNmDBBSUlJSk9PV1JSksaPH6+tW7c2dPPPW9XH4On/AQDAOoFWfvmoUaM0atSo8z6uVatWioyMrPGzuXPnasSIEZo1a5YkadasWdq0aZPmzp2rpUuX1qe59VelB4gRMAAArNMk5wD16dNHsbGxGj58uDZs2OD1WWpqqhITE73Krr/+em3ZssWXTTwnkz4gAAAsY2kP0PmKjY3VokWL1K9fPxUVFelvf/ubhg8fro0bN2ro0KGSpJycHEVHR3sdFx0drZycnDOet6ioSEVFRZ59l8t1Qdpv0AMEAECj0KQCUOfOndW5c2fPfkJCgjIzM/WnP/3JE4Ak7/V2pPInrk4vq2rOnDl67LHHGr7Bp2EONAAAjUOTHAKrKj4+Xnv27PHsx8TEVOvtOXToULVeoapmzZqlvLw8z5aZmXlB2nq2EAYAAHynyQegXbt2KTY21rOfkJCgdevWedX58MMPNWjQoDOew263KyIiwmu7ELzXAbogXwEAAGrB0iGwgoIC7d2717O/b98+paWlKSoqSu3atdOsWbOUlZWlJUuWSCp/wqt9+/bq3r27iouL9cYbbyglJUUpKSmec0yfPl1Dhw7Vs88+qxtvvFHvvvuu1q9fr08++cTn13c6rzlATIIGAMAylgag7du3a9iwYZ79mTNnSpImTZqk5ORkZWdnKyMjw/N5cXGx7r33XmVlZSkkJETdu3fXe++9p9GjR3vqDBo0SMuWLdPs2bP10EMP6fLLL9fy5cs1cOBA311YLdADBACAdQyTdzJU43K55HQ6lZeX16DDYSeLy9T14fclSZ8/dr2a2ZvUHHQAABq18/n73eTnADUlzIEGAKBxIABZhI43AACsQwDyIe9J0AAAwCoEIB/yehkqCQgAAMsQgKxCAAIAwDIEIB9iHSAAABoHApAP8RAYAACNAwHIh6q+C4w5QAAAWIcA5ENe7wKzrBUAAIAAZBHWAQIAwDoEIB9iHSAAABoHApAPGbwLAwCARoEAZBFGwAAAsA4ByMcqO4FYBwgAAOsQgHzMMwhG/gEAwDIEIIuQfwAAsA4ByMeYCA0AgPUIQD5WGX+YBA0AgHUIQD7GJGgAAKxHAPIxo6IPiB4gAACsQwCyCPkHAADrEIB8rXIIjC4gAAAsQwDyMZ4BAwDAegQgH/NMgqYDCAAAyxCAfMygDwgAAMsRgHyMHiAAAKxHALII6wABAGAdApCPMQAGAID1CEA+VvkuMIbAAACwDgHIxzzvArO0FQAA+DcCkK+xECIAAJYjAFmE+AMAgHUIQD7GJGgAAKxHAPIxJkEDAGA9ApCPGZ4uIBIQAABWIQD5mOcpMPIPAACWIQD5mGcIzOJ2AADgzwhAFqEHCAAA61gagDZv3qyxY8eqdevWMgxD77zzzlnrr1y5UiNGjNAll1yiiIgIJSQk6IMPPvCqk5ycLMMwqm2FhYUX8Epqj6fAAACwnqUB6Pjx4+rVq5cWLFhQq/qbN2/WiBEjtGbNGu3YsUPDhg3T2LFjtWvXLq96ERERys7O9tocDseFuITz5nkbPINgAABYJtDKLx81apRGjRpV6/pz58712n/66af17rvv6h//+If69OnjKTcMQzExMQ3VzAbGY/AAAFitSc8Bcrvdys/PV1RUlFd5QUGB4uLi1KZNG40ZM6ZaD5GVPD1ABCAAACzTpAPQc889p+PHj2v8+PGesi5duig5OVmrV6/W0qVL5XA4NHjwYO3Zs+eM5ykqKpLL5fLaLpRTL0MlAQEAYBVLh8DqY+nSpXr00Uf17rvvqlWrVp7y+Ph4xcfHe/YHDx6svn376vnnn9f8+fNrPNecOXP02GOPXfA2AwCAxqFJ9gAtX75cU6ZM0YoVK3Tdddedta7NZtNVV1111h6gWbNmKS8vz7NlZmY2dJM9GAIDAMB6Ta4HaOnSpfrlL3+ppUuX6oYbbjhnfdM0lZaWpiuvvPKMdex2u+x2e0M284wMHoQHAMBylgaggoIC7d2717O/b98+paWlKSoqSu3atdOsWbOUlZWlJUuWSCoPP7fffrvmzZun+Ph45eTkSJJCQkLkdDolSY899pji4+PVsWNHuVwuzZ8/X2lpaXrhhRd8f4E1oAcIAADrWToEtn37dvXp08fzCPvMmTPVp08fPfzww5Kk7OxsZWRkeOq//PLLKi0t1Z133qnY2FjPNn36dE+d3NxcTZ06VV27dlViYqKysrK0efNmDRgwwLcXdwZMggYAwHqGadIXcTqXyyWn06m8vDxFREQ06LkHzfmXDuYV6t07B6tX28gGPTcAAP7sfP5+N8lJ0E1Z5ctQAQCAdQhAFqHbDQAA6xCAfOzUJGgiEAAAViEA+dipl6ECAACrEIB8zOBlqAAAWI4ABAAA/A4ByMdOPQRGFxAAAFYhAPmYZyFE8g8AAJYhAPlY5TpA5B8AAKxDAPIxeoAAALAeAQgAAPgdApCvsRAiAACWIwD52Km3wQMAAKsQgHzMMwmaBAQAgGUIQD52qgeIBAQAgFUIQD5mMAYGAIDlCEAAAMDvEIB8zPMyVIvbAQCAPyMA+ZjheQze2nYAAODPCEAWYRI0AADWIQD5GI/BAwBgPQKQjxnnrgIAAC4wApBF6AACAMA6BCAfM3gXGAAAliMA+ZgnAFnbDAAA/BoByMcMkYAAALAaAcjHDGZBAwBgOQKQRVgHCAAA6xCAfMzzLlTyDwAAliEA+RoLIQIAYDkCkI95eoAsbQUAAP6NAORjrAMEAID1CEA+xkNgAABYjwBkEfp/AACwDgHIx2yeSdBEIAAArEIA8rHKAOQm/wAAYBkCkI/ZKu64mx4gAAAsQwDyscoeoDK6gAAAsIylAWjz5s0aO3asWrduLcMw9M4775zzmE2bNqlfv35yOBy67LLL9NJLL1Wrk5KSom7duslut6tbt25atWrVBWh93QTYKofACEAAAFjF0gB0/Phx9erVSwsWLKhV/X379mn06NEaMmSIdu3apQceeEC/+93vlJKS4qmTmpqqCRMmKCkpSenp6UpKStL48eO1devWC3UZ58UzB8htcUMAAPBjhtlIHkcyDEOrVq3SuHHjzljnvvvu0+rVq/Xll196yqZNm6b09HSlpqZKkiZMmCCXy6W1a9d66owcOVLNmzfX0qVLa9UWl8slp9OpvLw8RURE1O2CzuAXr2/Thq9/0B9v7qnx/ds26LkBAPBn5/P3u0nNAUpNTVViYqJX2fXXX6/t27erpKTkrHW2bNnis3aejWcIjDlAAABYJtDqBpyPnJwcRUdHe5VFR0ertLRUhw8fVmxs7Bnr5OTknPG8RUVFKioq8uy7XK6GbXgVPAYPAID1mlQPkFQ+VFZV5Qhe1fKa6pxeVtWcOXPkdDo9W9u2F25oyvMUWOMYeQQAwC81qQAUExNTrSfn0KFDCgwMVIsWLc5a5/ReoapmzZqlvLw8z5aZmdnwja/AEBgAANZrUgEoISFB69at8yr78MMP1b9/fwUFBZ21zqBBg854XrvdroiICK/tQrHxGDwAAJazdA5QQUGB9u7d69nft2+f0tLSFBUVpXbt2mnWrFnKysrSkiVLJJU/8bVgwQLNnDlTv/rVr5SamqrFixd7Pd01ffp0DR06VM8++6xuvPFGvfvuu1q/fr0++eQTn19fTSryDwshAgBgIUt7gLZv364+ffqoT58+kqSZM2eqT58+evjhhyVJ2dnZysjI8NTv0KGD1qxZo40bN6p379564oknNH/+fP30pz/11Bk0aJCWLVum119/XT179lRycrKWL1+ugQMH+vbiziDAoAcIAACrNZp1gBqTC7kO0L1vp+vvO77T/aO6aNo1lzfouQEA8GcXfB2gzMxMfffdd579bdu2acaMGVq0aFFdTudXGAIDAMB6dQpAt956qzZs2CCpfG2eESNGaNu2bXrggQf0+OOPN2gDLzY8BQYAgPXqFIA+//xzDRgwQJK0YsUK9ejRQ1u2bNFbb72l5OTkhmzfRYeFEAEAsF6dAlBJSYnsdrskaf369frxj38sSerSpYuys7MbrnUXIRZCBADAenUKQN27d9dLL72kjz/+WOvWrdPIkSMlSQcPHvQsSIiaMQQGAID16hSAnn32Wb388su69tprdcstt6hXr16SpNWrV3uGxlAzG4/BAwBguTothHjttdfq8OHDcrlcat68uad86tSpCg0NbbDGXYw8T4ERgAAAsEydeoBOnjypoqIiT/g5cOCA5s6dq6+//lqtWrVq0AZebBgCAwDAenUKQDfeeKPn9RS5ubkaOHCgnnvuOY0bN04LFy5s0AZebE69C8zihgAA4MfqFIB27typIUOGSJL+/ve/Kzo6WgcOHNCSJUs0f/78Bm3gxYaFEAEAsF6dAtCJEycUHh4uqfxN6zfddJNsNpvi4+N14MCBBm3gxYZ3gQEAYL06BaArrrhC77zzjjIzM/XBBx8oMTFRknTo0KEGf3fWxebUEBgBCAAAq9QpAD388MO699571b59ew0YMEAJCQmSynuDKt/sjpp5FkJ0W9wQAAD8WJ0eg7/55pt19dVXKzs727MGkCQNHz5cP/nJTxqscRcjngIDAMB6dQpAkhQTE6OYmBh99913MgxDl156KYsg1gILIQIAYL06DYG53W49/vjjcjqdiouLU7t27RQZGaknnnhCbjdjO2fDQogAAFivTj1ADz74oBYvXqxnnnlGgwcPlmma+ve//61HH31UhYWFeuqppxq6nRcNhsAAALBenQLQX//6V7366quet8BLUq9evXTppZfqt7/9LQHoLE4NgVncEAAA/FidhsCOHj2qLl26VCvv0qWLjh49Wu9GXcwYAgMAwHp1CkC9evXSggULqpUvWLBAPXv2rHejLmYMgQEAYL06DYH98Y9/1A033KD169crISFBhmFoy5YtyszM1Jo1axq6jRcVFkIEAMB6deoBuuaaa/TNN9/oJz/5iXJzc3X06FHddNNN+u9//6vXX3+9odt4UWEhRAAArFfndYBat25dbbJzenq6/vrXv+q1116rd8MuVrwLDAAA69WpBwh1V5F/CEAAAFiIAORjlZOgy5gEDQCAZQhAPhbAJGgAACx3XnOAbrrpprN+npubW5+2+AWjcg4Qk6ABALDMeQUgp9N5zs9vv/32ejXoYlc5CZqFEAEAsM55BSAeca+/gIpBRxZCBADAOswB8rGgigRUwkJAAABYhgDkY8GB5be8qJQABACAVQhAPkYPEAAA1iMA+VhlD1AxAQgAAMsQgHwsuKIHqJghMAAALEMA8jFPDxABCAAAyxCAfCzYMweIx+ABALAKAcjH6AECAMB6BCAfqzoJ2mQ1aAAALGF5AHrxxRfVoUMHORwO9evXTx9//PEZ606ePFmGYVTbunfv7qmTnJxcY53CwkJfXM45VT4GLzEMBgCAVSwNQMuXL9eMGTP04IMPateuXRoyZIhGjRqljIyMGuvPmzdP2dnZni0zM1NRUVH62c9+5lUvIiLCq152drYcDocvLumc7IGnbjmPwgMAYA1LA9Cf//xnTZkyRXfccYe6du2quXPnqm3btlq4cGGN9Z1Op2JiYjzb9u3bdezYMf3iF7/wqmcYhle9mJgYX1xOrQRX6QFiHhAAANawLAAVFxdrx44dSkxM9CpPTEzUli1banWOxYsX67rrrlNcXJxXeUFBgeLi4tSmTRuNGTNGu3btarB215fNZijQVv5GeAIQAADWsCwAHT58WGVlZYqOjvYqj46OVk5OzjmPz87O1tq1a3XHHXd4lXfp0kXJyclavXq1li5dKofDocGDB2vPnj1nPFdRUZFcLpfXdiHxOgwAAKxl+SRowzC89k3TrFZWk+TkZEVGRmrcuHFe5fHx8Zo4caJ69eqlIUOGaMWKFerUqZOef/75M55rzpw5cjqdnq1t27Z1upba4oWoAABYy7IA1LJlSwUEBFTr7Tl06FC1XqHTmaap1157TUlJSQoODj5rXZvNpquuuuqsPUCzZs1SXl6eZ8vMzKz9hdQBawEBAGAtywJQcHCw+vXrp3Xr1nmVr1u3ToMGDTrrsZs2bdLevXs1ZcqUc36PaZpKS0tTbGzsGevY7XZFRER4bReS531gDIEBAGCJQCu/fObMmUpKSlL//v2VkJCgRYsWKSMjQ9OmTZNU3jOTlZWlJUuWeB23ePFiDRw4UD169Kh2zscee0zx8fHq2LGjXC6X5s+fr7S0NL3wwgs+uabacASVB6DCkjKLWwIAgH+yNABNmDBBR44c0eOPP67s7Gz16NFDa9as8TzVlZ2dXW1NoLy8PKWkpGjevHk1njM3N1dTp05VTk6OnE6n+vTpo82bN2vAgAEX/HpqKyQ4QJJ0kgAEAIAlDJP3MVTjcrnkdDqVl5d3QYbDxr+Uqm37j+rF2/pq9JVnHpoDAAC1dz5/vy1/CswfVfYAnSimBwgAACsQgCwQEsQQGAAAViIAWSC0cg5QcanFLQEAwD8RgCzg8AQgHoMHAMAKBCALhFYMgZ0ooQcIAAArEIAscGoIjDlAAABYgQBkAQcBCAAASxGALOAZAiMAAQBgCQKQBcIdQZKk/CLmAAEAYAUCkAUiQsoDUN7JEotbAgCAfyIAWcBZEYDyCUAAAFiCAGSBiJDyd9DSAwQAgDUIQBZwVhkC4120AAD4HgHIApUBqNRt8j4wAAAsQACyQEhQgAJthiSGwQAAsAIByAKGYXgNgwEAAN8iAFmk8lF410nWAgIAwNcIQBZhLSAAAKxDALJIhKP8UXgXAQgAAJ8jAFmEOUAAAFiHAGQRhsAAALAOAcgi9AABAGAdApBFLmlmlyT9kF9kcUsAAPA/BCCLREc4JEnfuwotbgkAAP6HAGSRGGd5D9D3+QQgAAB8jQBkkVbhlT1ARbwQFQAAHyMAWaRVRHkPUHGpW7knmAgNAIAvEYAsYg8MUFRYsCQph3lAAAD4FAHIQq3CK+YBEYAAAPApApCFYpzl84AOuXgUHgAAXyIAWSi2IgBl5Z60uCUAAPgXApCF2rcIkyR9e/i4xS0BAMC/EIAsdPklzSRJ/ztUYHFLAADwLwQgC13eqjwAfXu4QG43awEBAOArBCALtW0eoqAAQ4Ulbh3MYx4QAAC+QgCyUGCAzTMP6H8/MA8IAABfIQBZrGN0+TDYl9kui1sCAID/IABZrMelTknS7qw8i1sCAID/IABZrFebSEnS7u8IQAAA+IrlAejFF19Uhw4d5HA41K9fP3388cdnrLtx40YZhlFt++qrr7zqpaSkqFu3brLb7erWrZtWrVp1oS+jznq0Lu8Byjh6Qrknii1uDQAA/sHSALR8+XLNmDFDDz74oHbt2qUhQ4Zo1KhRysjIOOtxX3/9tbKzsz1bx44dPZ+lpqZqwoQJSkpKUnp6upKSkjR+/Hht3br1Ql9OnThDg9S+RagkKS0z19rGAADgJwzTNC1bgGbgwIHq27evFi5c6Cnr2rWrxo0bpzlz5lSrv3HjRg0bNkzHjh1TZGRkjeecMGGCXC6X1q5d6ykbOXKkmjdvrqVLl9aqXS6XS06nU3l5eYqIiDi/i6qDe1akK2Xnd/rNtZfrvpFdLvj3AQBwMTqfv9+W9QAVFxdrx44dSkxM9CpPTEzUli1bznpsnz59FBsbq+HDh2vDhg1en6WmplY75/XXX3/WcxYVFcnlcnltvpRweQtJUur/jvj0ewEA8FeWBaDDhw+rrKxM0dHRXuXR0dHKycmp8ZjY2FgtWrRIKSkpWrlypTp37qzhw4dr8+bNnjo5OTnndU5JmjNnjpxOp2dr27ZtPa7s/FUGoN1ZecovLPHpdwMA4I8CrW6AYRhe+6ZpViur1LlzZ3Xu3Nmzn5CQoMzMTP3pT3/S0KFD63ROSZo1a5Zmzpzp2Xe5XD4NQZdGhqh9i1DtP3JCn+w5rFFXxvrsuwEA8EeW9QC1bNlSAQEB1XpmDh06VK0H52zi4+O1Z88ez35MTMx5n9NutysiIsJr87XE7jGSpLWfn7mnCgAANAzLAlBwcLD69eundevWeZWvW7dOgwYNqvV5du3apdjYUz0mCQkJ1c754Ycfntc5rXB9RQD66KtDKiots7g1AABc3CwdAps5c6aSkpLUv39/JSQkaNGiRcrIyNC0adMklQ9NZWVlacmSJZKkuXPnqn379urevbuKi4v1xhtvKCUlRSkpKZ5zTp8+XUOHDtWzzz6rG2+8Ue+++67Wr1+vTz75xJJrrK0+bSMVHWHX964ibdl7RMO6tLK6SQAAXLQsDUATJkzQkSNH9Pjjjys7O1s9evTQmjVrFBcXJ0nKzs72WhOouLhY9957r7KyshQSEqLu3bvrvffe0+jRoz11Bg0apGXLlmn27Nl66KGHdPnll2v58uUaOHCgz6/vfNhshq7vHqMlqQe0Ov0gAQgAgAvI0nWAGitfrwNUaWfGMd304hY5gmz67MHrFO4I8tl3AwDQ1DWJdYBQXZ+2kbr8kjAVlrj13n+yrW4OAAAXLQJQI2IYhn7Wv/zx+7d3fGdxawAAuHgRgBqZn/S5VDZD2nHgmP73Q4HVzQEA4KJEAGpkoiMcGta5fAL031IPWNwaAAAuTgSgRugXgztIklZsz1TeSV6NAQBAQyMANUKDr2ihLjHhOlFcpmXbMs59AAAAOC8EoEbIMAz98uryXqDkLftVUua2uEUAAFxcCECN1I29W6tlM7uy8wq1ZjePxAMA0JAIQI2UPTBAtyeUr4i9cOP/5HazXiUAAA2FANSITUpor3B7oL7KydcH/+Ut8QAANBQCUCPmDA3SLyrmAs371x56gQAAaCAEoEZuyuAO9AIBANDACECNHL1AAAA0PAJQE1C1F+jDL+gFAgCgvghATYAzNEi/GNxekjR3Pb1AAADUFwGoifjl1ad6gd5Nz7K6OQAANGkEoCYiMjRYvxl2uSTpmbVf6XhRqcUtAgCg6SIANSFTru6guBah+t5VpBc27LW6OQAANFkEoCbEHhig2Td0kyS9+vE+7T983OIWAQDQNBGAmpjrurbSkI4tVVzm1hP//EKmyYRoAADOFwGoiTEMQ4+M7aZAm6F/fXVIq9MPWt0kAACaHAJQE3RFq3D9bnhHSdLD7/5Xh1yFFrcIAICmhQDURP3m2svV49II5Z0s0QOrdjMUBgDAeSAANVFBATY997PeCgowtP7LQ0rZydpAAADUFgGoCescE64Z13WSJD387uf69ocCi1sEAEDTQABq4qZdc7niL4vSieIy3fnWLhWWlFndJAAAGj0CUBMXYDM07+d91CIsWF9mu/TUe19a3SQAABo9AtBFIDrCoefG95Ik/e3TA1q16zuLWwQAQONGALpIXNu5le6seFfYfX/frc/2H7W4RQAANF4EoIvIPSM6a2T3GBWXuTV1yXZelQEAwBkQgC4iNpuhv0zorZ5tnDp2okQTF29VVu5Jq5sFAECjQwC6yIQEB+jVSf3VoWWYvjt2Ure98qm+Z6VoAAC8EIAuQq3CHXrrVwPVNipE+4+c0K2vfKof8ousbhYAAI0GAegiFesM0Vt3xKu106H//XBcP1+UqoMMhwEAIIkAdFFrGxWqt351KgTdvHCL9h5itWgAAAhAF7n2LcP0998M0uWXhOlgXqF+9tIWpWfmWt0sAAAsRQDyA60jQ/T2tEHqVfF02C2vfKqNXx+yulkAAFiGAOQnosKC9eav4nX1FS11orhMU/66Xcu2ZVjdLAAALGF5AHrxxRfVoUMHORwO9evXTx9//PEZ665cuVIjRozQJZdcooiICCUkJOiDDz7wqpOcnCzDMKpthYU8Ct7MHqjXJl+lm/pcqjK3qftX7tafPvhapmla3TQAAHzK0gC0fPlyzZgxQw8++KB27dqlIUOGaNSoUcrIqLlnYvPmzRoxYoTWrFmjHTt2aNiwYRo7dqx27drlVS8iIkLZ2dlem8Ph8MUlNXrBgTY9N76Xfje8oyRpwYa9+v3yNBWV8hZ5AID/MEwL/9//gQMHqm/fvlq4cKGnrGvXrho3bpzmzJlTq3N0795dEyZM0MMPPyypvAdoxowZys3NrXO7XC6XnE6n8vLyFBERUefzNHYrPsvUA6t2q9RtamCHKC1K6i9naJDVzQIAoE7O5++3ZT1AxcXF2rFjhxITE73KExMTtWXLllqdw+12Kz8/X1FRUV7lBQUFiouLU5s2bTRmzJhqPUSnKyoqksvl8tr8wfir2uq1yVepmT1QW/cd1U0L/63MoyesbhYAABecZQHo8OHDKisrU3R0tFd5dHS0cnJyanWO5557TsePH9f48eM9ZV26dFFycrJWr16tpUuXyuFwaPDgwdqzZ88ZzzNnzhw5nU7P1rZt27pdVBM0tNMl+vtvEhRbsVbQjS/8mzfJAwAuepZPgjYMw2vfNM1qZTVZunSpHn30US1fvlytWrXylMfHx2vixInq1auXhgwZohUrVqhTp056/vnnz3iuWbNmKS8vz7NlZmbW/YKaoC4xEVr128HqcWmEjh4v1m2vbFXKju+sbhYAABeMZQGoZcuWCggIqNbbc+jQoWq9Qqdbvny5pkyZohUrVui66647a12bzaarrrrqrD1AdrtdERERXpu/iXE6tOLXCRrZPUbFZW7d83a6nl7zpUrL3FY3DQCABmdZAAoODla/fv20bt06r/J169Zp0KBBZzxu6dKlmjx5st566y3dcMMN5/we0zSVlpam2NjYerf5YhcaHKgXb+uru4ZdIUlatPlb3frKVuXksYQAAODiYukQ2MyZM/Xqq6/qtdde05dffqnf//73ysjI0LRp0ySVD03dfvvtnvpLly7V7bffrueee07x8fHKyclRTk6O8vLyPHUee+wxffDBB/r222+VlpamKVOmKC0tzXNOnJ3NZuje6zvrxdv6qpk9UNv2H9UN8z/Wx3t+sLppAAA0GEsD0IQJEzR37lw9/vjj6t27tzZv3qw1a9YoLi5OkpSdne21JtDLL7+s0tJS3XnnnYqNjfVs06dP99TJzc3V1KlT1bVrVyUmJiorK0ubN2/WgAEDfH59TdnoK2P1z7uvVrfYCB05XqzbX9umv6z7RmVuFk0EADR9lq4D1Fj5yzpAtVFYUqbH/vGFlla8NqNfXHP9eXwvxbUIs7hlAAB4axLrAKFpcAQFaM5NV2rez3sr3B6oHQeOadS8j7V0Wwav0AAANFkEINTKjb0v1doZQxR/WZROFJdp1srduvWVrdp7KN/qpgEAcN4IQKi1Ns1D9dYd8Zp9Q1fZA21K/faIRs37WM+s/UrHi0qtbh4AALVGAMJ5sdkM3THkMq2feY2u69pKJWWmXtr0P137p41649MDKmHdIABAE8Ak6BowCbr21n3xvZ745xfKqHiH2GUtw/R/13fWyB4xtVrRGwCAhnI+f78JQDUgAJ2f4lK33tp6QPM/2qujx4slSb3bRuqexE66+oqWBCEAgE8QgOqJAFQ3+YUlemXzt3rl4306WVImSbqqfXP9fkQnDbq8pcWtAwBc7AhA9UQAqp9D+YV6aeO3emPrARWXls8Jir8sSjNHdNaADlEWtw4AcLEiANUTAahh5OQVauHGvVq6LVPFFZOjr76ipX4/opP6xTW3uHUAgIsNAaieCEAN62DuSb2wYa9WbM9USVn5P7drOl2iu350hfrHNWeOEACgQRCA6okAdGFkHj2hFzbs1ds7vvO8U6zHpRH6xaAOGtMrVvbAAItbCABoyghA9UQAurAOHDmuhRv/p1W7slRUMUeoZbNg3TowThPj26lVuMPiFgIAmiICUD0RgHzj6PFiLd2Wob+lHlCOq1CSFBRgaEzP1po8qL16tY20toEAgCaFAFRPBCDfKilz6/3Pc5S8Zb92HDjmKe/bLlK/GNxBI3vEKCiARcsBAGdHAKonApB10jNzlbxlv/75n4OeCdOXhNs1tmdr/bh3a/Vq42TSNACgRgSgeiIAWe+Qq1BvbM3QW1sP6HBBsac8rkWoxvSM1XVdo9WrTaRsNsIQAKAcAaieCECNR3GpW5u/+UHvph/U+i++96wwLZVPnL62cysN6dhSAzpEKdYZYmFLAQBWIwDVEwGocTpeVKr1X36vD7/4Xpu//kH5RaVen7eNCtFV7aM0sEOUrmofpQ4twxguAwA/QgCqJwJQ41dS5tZn+49qw1eHtHXfUX2elSf3af+SnSFB6nFphHpc6tSVlzrV89JItY0KIRQBwEWKAFRPBKCmp6CoVDsPHNO2fUe1bd9RpX2X63kPWVWnh6IuMRFq3yJUgTxlBgBNHgGonghATV9xqVvffJ+v3Vl52p2Vp8+z8vRVdr7nnWRVBQfYdNklYbqiVTN1ig5Xp+hm6hgdrrgoghEANCUEoHoiAF2cTg9F/83K0zffF3hNrK4q0GaobVSo4lqEqn2LsPKfLcPUvkWY2jQPYW0iAGhkCED1RADyH263qazck9pzKF/ffF+gb77P157vC7T30JmDkSQF2AxdGhniFY5inSGKjXSotTNEl4TbFcAj+gDgUwSgeiIAwe029X1+ofYfPqH9R45r/5HjOlDx+4EjJ84ajqTy3qPoCIdinQ7FRoYo1ulQq3C7LqnYyn93KMIRyKRsAGggBKB6IgDhbEzT1A/5Rdp3uDwM7T9yXJnHTio796Sy8wqV4yr0vO3+XOyBttNCkV2twh3lZc3sah4WJGdIsJqHBskZEsScJAA4i/P5+x3oozYBFw3DMNQqwqFWEQ4NvKxFtc/L3OUB6WDeSWXnFio7rzwY/ZBfpB/yi3Qov/x3V2Gpikrd+u7YSX137GStvjvcHihnaJCahwYrsiIURYYGKTKkfD8yNFiRIUGKCAlSM3ugwh2BamYPVDNHIHOWAKAKAhDQwAJshmKcDsU4HVK7M9crLCmrCERFFeGo0Gv/cEGRck+W6NjxYrkKyxd9zC8qVX5Raa0DU1WOIJua2YNOhaKKYBRe+dMRqGb2IDVzBCo0KEChwQFyBAdU/B6okGCbQoLLPwsJDpA90MbwHYAmiwAEWMQRFKC2UaFqGxV6zrplblOukyXlgehEsfJOlCj3ZLFyT5To2IkS5Z0oVu7JEuWeKFHuiWLlF5YHpYLCUs98pcIStwpLyoNVQ7AZUkhQgEIqwlFoUKBCggPkCLLJHlgekBxB5T/tVcrsgQEV+1XKvD63yR50ht8DAxQUYBC8ANQbAQhoAgJshpqHBat5WLA6KOy8ji0tc6ugqFT5haUqKKrYKgJSfmGJCirK8z0/S3SiuEyFJWU6UVymk8VlOlnl98q1lNymdLy4TMeLzz4hvKEZhhQUYFNwgE1BAYaCA21V9m0KCjTKf1apU15+lmMqjvPar1Kvcj/QZlNggKFAm6HAAFvFz4p9m00BtvLvKv9peO0H2ghuQGNCAAIucoEBtvK5QaHBDXK+0jK3TpZ4B6OqgamwpExFpW4VlZapqMTt+b2wpKKs1F1RXlnPraKSKr9XPa6ivOoClqZZvqZTTSt9N3aVQSjQZpwWlipCUpUwdfrv1cNVeQCzGYYCbOXnDrAZCjAM2Sp+eso89Ywa6kkBAbaK+pLNKG+Hp36V48523kCv76g4j80mW2XbTjuPzSgvM4zyurbKnywfAR8hAAE4L4EBNoUH2BTuCPLZd7rdporLTgWn4jK3SspMlZSVB6GSqvtlbpWUeu+fqlNeXm3fc8yp/ap1istMlZS6VeY2Vep2q9RtqrSs/Pcyt6mSMrPiZ2Wdmp8CLHOX12uYQciLlycMGYZstlO/G4Y8octmlD+QYDNUEaRO1fUOVpWBS17HBdhOO4et8jtOO6cnmFU5X5WwVvm7USVA2qp8f4DNO+QZKv+pKucxJE/wO3VseV2jShsr65Uffuq8RsVx8jpfTcdXXnPF8V7fd9rxRmU7vdt4pu82Ks8rw/PdZ/0uSfYgm1qFO3z8r+sUAhCARs9mM+SwBcgRFCDJd8GrrkzT9ASh8rDkHZrKf3r/XuZ2VwtSlfuV9crcpkq8QpdbZW7JXfF9ns005a7ye2X5qXoqP9bUOeqddj6zyrFuU25TZ6lX/Xy1XB1CbrP8miRT8u0IK3yoT7tIrfrtYMu+nwAEAA3MqBhGCgywuiWNS2UwLDNNmRUhpzLsmG6VB6iKzawIV+4qdStDlFnluDJ31XOZp87pPlWn6vdULa88T03fU7V9ZZV1z3DOqm2tbJ93W099T+V98BxbsW9WOZfblEyV/15Z11RFKPSqV8Pxkud7zYrzuN0VPysy5al6p76j6n7V46vWrfnYqm2svOaazln9eHugtUtzEIAAAD7hCYZWNwSQxMpoAADA7xCAAACA3yEAAQAAv0MAAgAAfsfyAPTiiy+qQ4cOcjgc6tevnz7++OOz1t+0aZP69esnh8Ohyy67TC+99FK1OikpKerWrZvsdru6deumVatWXajmAwCAJsjSALR8+XLNmDFDDz74oHbt2qUhQ4Zo1KhRysjIqLH+vn37NHr0aA0ZMkS7du3SAw88oN/97ndKSUnx1ElNTdWECROUlJSk9PR0JSUlafz48dq6dauvLgsAADRyhmmatVyaquENHDhQffv21cKFCz1lXbt21bhx4zRnzpxq9e+77z6tXr1aX375pads2rRpSk9PV2pqqiRpwoQJcrlcWrt2rafOyJEj1bx5cy1durRW7XK5XHI6ncrLy1NERERdLw8AAPjQ+fz9tqwHqLi4WDt27FBiYqJXeWJiorZs2VLjMampqdXqX3/99dq+fbtKSkrOWudM55SkoqIiuVwurw0AAFy8LAtAhw8fVllZmaKjo73Ko6OjlZOTU+MxOTk5NdYvLS3V4cOHz1rnTOeUpDlz5sjpdHq2tm3b1uWSAABAE2H5JGjD8H7zr2ma1crOVf/08vM956xZs5SXl+fZMjMza91+AADQ9Fi2InnLli0VEBBQrWfm0KFD1XpwKsXExNRYPzAwUC1atDhrnTOdU5LsdrvsdntdLgMAADRBlvUABQcHq1+/flq3bp1X+bp16zRo0KAaj0lISKhW/8MPP1T//v0VFBR01jpnOicAAPA/lr6TbubMmUpKSlL//v2VkJCgRYsWKSMjQ9OmTZNUPjSVlZWlJUuWSCp/4mvBggWaOXOmfvWrXyk1NVWLFy/2erpr+vTpGjp0qJ599lndeOONevfdd7V+/Xp98sknllwjAABofCwNQBMmTNCRI0f0+OOPKzs7Wz169NCaNWsUFxcnScrOzvZaE6hDhw5as2aNfv/73+uFF15Q69atNX/+fP30pz/11Bk0aJCWLVum2bNn66GHHtLll1+u5cuXa+DAgT6/PgAA0DhZug5QY5WXl6fIyEhlZmayDhAAAE2Ey+VS27ZtlZubK6fTeda6lvYANVb5+fmSxOPwAAA0Qfn5+ecMQPQA1cDtduvgwYMKDw8/6+PzdVGZTuldurC4z77BffYd7rVvcJ9940LdZ9M0lZ+fr9atW8tmO/tzXvQA1cBms6lNmzYX9DsiIiL4Py4f4D77BvfZd7jXvsF99o0LcZ/P1fNTyfKFEAEAAHyNAAQAAPwOAcjH7Ha7HnnkEVaevsC4z77BffYd7rVvcJ99ozHcZyZBAwAAv0MPEAAA8DsEIAAA4HcIQAAAwO8QgAAAgN8hAPnQiy++qA4dOsjhcKhfv376+OOPrW5SkzJnzhxdddVVCg8PV6tWrTRu3Dh9/fXXXnVM09Sjjz6q1q1bKyQkRNdee63++9//etUpKirS3XffrZYtWyosLEw//vGP9d133/nyUpqUOXPmyDAMzZgxw1PGfW4YWVlZmjhxolq0aKHQ0FD17t1bO3bs8HzOfW4YpaWlmj17tjp06KCQkBBddtllevzxx+V2uz11uNfnb/PmzRo7dqxat24twzD0zjvveH3eUPf02LFjSkpKktPplNPpVFJSknJzc+t/ASZ8YtmyZWZQUJD5yiuvmF988YU5ffp0MywszDxw4IDVTWsyrr/+evP11183P//8czMtLc284YYbzHbt2pkFBQWeOs8884wZHh5upqSkmLt37zYnTJhgxsbGmi6Xy1Nn2rRp5qWXXmquW7fO3Llzpzls2DCzV69eZmlpqRWX1aht27bNbN++vdmzZ09z+vTpnnLuc/0dPXrUjIuLMydPnmxu3brV3Ldvn7l+/Xpz7969njrc54bx5JNPmi1atDD/+c9/mvv27TPffvtts1mzZubcuXM9dbjX52/NmjXmgw8+aKakpJiSzFWrVnl93lD3dOTIkWaPHj3MLVu2mFu2bDF79Ohhjhkzpt7tJwD5yIABA8xp06Z5lXXp0sW8//77LWpR03fo0CFTkrlp0ybTNE3T7XabMTEx5jPPPOOpU1hYaDqdTvOll14yTdM0c3NzzaCgIHPZsmWeOllZWabNZjPff/99315AI5efn2927NjRXLdunXnNNdd4AhD3uWHcd9995tVXX33Gz7nPDeeGG24wf/nLX3qV3XTTTebEiRNN0+ReN4TTA1BD3dMvvvjClGR++umnnjqpqammJPOrr76qV5sZAvOB4uJi7dixQ4mJiV7liYmJ2rJli0Wtavry8vIkSVFRUZKkffv2KScnx+s+2+12XXPNNZ77vGPHDpWUlHjVad26tXr06MF/i9PceeeduuGGG3Tdddd5lXOfG8bq1avVv39//exnP1OrVq3Up08fvfLKK57Puc8N5+qrr9a//vUvffPNN5Kk9PR0ffLJJxo9erQk7vWF0FD3NDU1VU6nUwMHDvTUiY+Pl9PprPd952WoPnD48GGVlZUpOjraqzw6Olo5OTkWtappM01TM2fO1NVXX60ePXpIkude1nSfDxw44KkTHBys5s2bV6vDf4tTli1bpp07d+qzzz6r9hn3uWF8++23WrhwoWbOnKkHHnhA27Zt0+9+9zvZ7Xbdfvvt3OcGdN999ykvL09dunRRQECAysrK9NRTT+mWW26RxL/pC6Gh7mlOTo5atWpV7fytWrWq930nAPmQYRhe+6ZpVitD7dx11136z3/+o08++aTaZ3W5z/y3OCUzM1PTp0/Xhx9+KIfDccZ63Of6cbvd6t+/v55++mlJUp8+ffTf//5XCxcu1O233+6px32uv+XLl+uNN97QW2+9pe7duystLU0zZsxQ69atNWnSJE897nXDa4h7WlP9hrjvDIH5QMuWLRUQEFAtrR46dKhaOsa53X333Vq9erU2bNigNm3aeMpjYmIk6az3OSYmRsXFxTp27NgZ6/i7HTt26NChQ+rXr58CAwMVGBioTZs2af78+QoMDPTcJ+5z/cTGxqpbt25eZV27dlVGRoYk/j03pP/7v//T/fffr5///Oe68sorlZSUpN///veaM2eOJO71hdBQ9zQmJkbff/99tfP/8MMP9b7vBCAfCA4OVr9+/bRu3Tqv8nXr1mnQoEEWtarpMU1Td911l1auXKmPPvpIHTp08Pq8Q4cOiomJ8brPxcXF2rRpk+c+9+vXT0FBQV51srOz9fnnn/PfosLw4cO1e/dupaWlebb+/fvrtttuU1pami677DLucwMYPHhwtWUcvvnmG8XFxUni33NDOnHihGw27z93AQEBnsfgudcNr6HuaUJCgvLy8rRt2zZPna1btyovL6/+971eU6hRa5WPwS9evNj84osvzBkzZphhYWHm/v37rW5ak/Gb3/zGdDqd5saNG83s7GzPduLECU+dZ555xnQ6nebKlSvN3bt3m7fcckuNj122adPGXL9+vblz507zRz/6kV8/ylobVZ8CM03uc0PYtm2bGRgYaD711FPmnj17zDfffNMMDQ0133jjDU8d7nPDmDRpknnppZd6HoNfuXKl2bJlS/MPf/iDpw73+vzl5+ebu3btMnft2mVKMv/85z+bu3bt8izv0lD3dOTIkWbPnj3N1NRUMzU11bzyyit5DL6peeGFF8y4uDgzODjY7Nu3r+fxbdSOpBq3119/3VPH7XabjzzyiBkTE2Pa7XZz6NCh5u7du73Oc/LkSfOuu+4yo6KizJCQEHPMmDFmRkaGj6+maTk9AHGfG8Y//vEPs0ePHqbdbje7dOliLlq0yOtz7nPDcLlc5vTp08127dqZDofDvOyyy8wHH3zQLCoq8tThXp+/DRs21Pi/yZMmTTJNs+Hu6ZEjR8zbbrvNDA8PN8PDw83bbrvNPHbsWL3bb5imadavDwkAAKBpYQ4QAADwOwQgAADgdwhAAADA7xCAAACA3yEAAQAAv0MAAgAAfocABAAA/A4BCABqwTAMvfPOO1Y3A0ADIQABaPQmT54swzCqbSNHjrS6aQCaqECrGwAAtTFy5Ei9/vrrXmV2u92i1gBo6ugBAtAk2O12xcTEeG3NmzeXVD48tXDhQo0aNUohISHq0KGD3n77ba/jd+/erR/96EcKCQlRixYtNHXqVBUUFHjVee2119S9e3fZ7XbFxsbqrrvu8vr88OHD+slPfqLQ0FB17NhRq1evvrAXDeCCIQABuCg89NBD+ulPf6r09HRNnDhRt9xyi7788ktJ0okTJzRy5Eg1b95cn332md5++22tX7/eK+AsXLhQd955p6ZOnardu3dr9erVuuKKK7y+47HHHtP48eP1n//8R6NHj9Ztt92mo0eP+vQ6ATSQer9OFQAusEmTJpkBAQFmWFiY1/b444+bpmmaksxp06Z5HTNw4EDzN7/5jWmaprlo0SKzefPmZkFBgefz9957z7TZbGZOTo5pmqbZunVr88EHHzxjGySZs2fP9uwXFBSYhmGYa9eubbDrBOA7zAEC0CQMGzZMCxcu9CqLiory/J6QkOD1WUJCgtLS0iRJX375pXr16qWwsDDP54MHD5bb7dbXX38twzB08OBBDR8+/Kxt6Nmzp+f3sLAwhYeH69ChQ3W9JAAWIgABaBLCwsKqDUmdi2EYkiTTND2/11QnJCSkVucLCgqqdqzb7T6vNgFoHJgDBOCi8Omnn1bb79KliySpW7duSktL0/Hjxz2f//vf/5bNZlOnTp0UHh6u9u3b61//+pdP2wzAOvQAAWgSioqKlJOT41UWGBioli1bSpLefvtt9e/fX1dffbXefPNNbdu2TYsXL5Yk3XbbbXrkkUc0adIkPfroo/rhhx909913KykpSdHR0ZKkRx99VNOmTVOrVq00atQo5efn69///rfuvvtu314oAJ8gAAFoEt5//33FxsZ6lXXu3FlfffWVpPIntJYtW6bf/va3iomJ0Ztvvqlu3bpJkkJDQ/XBBx9o+vTpuuqqqxQaGqqf/vSn+vOf/+w516RJk1RYWKi//OUvuvfee9WyZUvdfPPNvrtAAD5lmKZpWt0IAKgPwzC0atUqjRs3zuqmAGgimAMEAAD8DgEIAAD4HeYAAWjyGMkHcL7oAQIAAH6HAAQAAPwOAQgAAPgdAhAAAPA7BCAAAOB3CEAAAMDvEIAAAIDfIQABAAC/QwACAAB+5/8BqKwoTuqVwbUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('iris.csv')\n",
    "outputs = df['species']\n",
    "unique_outputs = outputs.unique()\n",
    "test_sample = np.array([[6.3,2.7,4.9,1.8]])\n",
    "# unique_outputs[nn.predict(test_sample)]\n",
    "nn.predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(nn, 'model_weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with loaded model: 70.0000\n"
     ]
    }
   ],
   "source": [
    "loaded_nn = load_model('model_weights.npy')\n",
    "y_pred_loaded = loaded_nn.predict(X_test)\n",
    "print(f\"Accuracy with loaded model: {accuracy(y_pred_loaded, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
