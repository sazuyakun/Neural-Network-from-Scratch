{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Neural Network\n",
    "A neural network consists of:\n",
    "\n",
    "* `Layers`: Each layer has weights, biases, and an activation function.\n",
    "* `Forward Propagation`: Computes the output by passing input through the layers.\n",
    "* `Loss Function`: Measures the error between predicted and true outputs (e.g., cross-entropy for classification, MSE for regression).\n",
    "* `Backpropagation`: Computes gradients of the loss with respect to weights and biases.\n",
    "* `Gradient Descent`: Updates weights and biases to minimize the loss.\n",
    "\n",
    "Weâ€™ll implement a fully connected (dense) neural network with customizable layers and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing the Code\n",
    "File 1: `layers.py`\n",
    "This file defines a Layer class to represent a single layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The Layer class initializes weights and biases randomly.\n",
    "* `forward`: Computes the output of the layer (linear transformation + activation).\n",
    "* `backward`: Computes gradients and updates weights/biases using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation = activation  # Activation function (e.g., 'relu', 'sigmoid', 'softmax')\n",
    "        # Store intermediate values for backpropagation\n",
    "        self.input = None\n",
    "        self.z = None  # Pre-activation output\n",
    "        self.a = None  # Post-activation output\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass: X is the input (batch_size, input_size)\n",
    "        self.input = X\n",
    "        self.z = np.dot(X, self.weights) + self.biases  # Linear transformation\n",
    "        if self.activation == 'relu':\n",
    "            self.a = relu(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.a = sigmoid(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = softmax(self.z)\n",
    "        else:\n",
    "            self.a = self.z  # No activation\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        # Backward pass: delta is the gradient from the next layer\n",
    "        if self.activation == 'relu':\n",
    "            delta = delta * relu_derivative(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            delta = delta * sigmoid_derivative(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Softmax derivative is handled in the loss function (cross-entropy)\n",
    "            pass\n",
    "\n",
    "        # Compute gradients\n",
    "        dW = np.dot(self.input.T, delta)  # Gradient w.r.t weights\n",
    "        db = np.sum(delta, axis=0, keepdims=True)  # Gradient w.r.t biases\n",
    "        dX = np.dot(delta, self.weights.T)  # Gradient w.r.t input (for previous layer)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [[ 0. -1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1.  0. -1.]]\n",
      "(5, 3)\n",
      "\n",
      "bias: [[1. 1. 1.]]\n",
      "(1, 3)\n",
      "\n",
      "X: [[ 0.  0. -1. -1.  1.]\n",
      " [-1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  0.]\n",
      " [ 0.  0. -1.  0. -2.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [-1.  0. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1.]\n",
      " [ 0. -2.  0.  0.  0.]\n",
      " [-1.  0. -1.  0. -1.]]\n",
      "(10, 5)\n",
      "\n",
      "z: [[2. 3. 0.]\n",
      " [2. 3. 2.]\n",
      " [1. 1. 1.]\n",
      " [3. 3. 1.]\n",
      " [4. 2. 3.]\n",
      " [1. 1. 1.]\n",
      " [3. 4. 2.]\n",
      " [5. 3. 2.]\n",
      " [3. 1. 1.]\n",
      " [3. 3. 3.]]\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.random.randn(5, 3) // 2\n",
    "bias = np.ones((1, 3))\n",
    "X = np.random.randn(10, 5) // 2 # (batch_size, input_size)\n",
    "z = np.dot(X, weights) + bias # Linear transformation\n",
    "print(f\"weights: {weights}\")\n",
    "print(weights.shape)\n",
    "print(f\"\\nbias: {bias}\")\n",
    "print(bias.shape)\n",
    "print(f\"\\nX: {X}\")\n",
    "print(X.shape)\n",
    "print(f\"\\nz: {z}\")\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[0 0 3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "delta = np.array([1, 2, 3])\n",
    "z = np.array([-1, 0, 1])\n",
    "delta = delta * relu_derivative(z)\n",
    "print(relu_derivative(z))\n",
    "print(delta)\n",
    "\n",
    "db = np.sum(delta, axis=0, keepdims=True)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, -1,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3]])\n",
    "x - np.max(x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 2: `activations.py`\n",
    "\n",
    "Move the activation functions and their derivatives into a separate file for reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 3: `model.py`\n",
    "\n",
    "This file defines the NeuralNetwork class, which combines layers and implements training logic.\n",
    "### Explanation:\n",
    "\n",
    "* `add_layer`: Adds a layer to the network.\n",
    "* `forward`: Passes input through all layers.\n",
    "* `compute_loss`: Implements cross-entropy (for classification) and MSE (for regression).\n",
    "* `backward`: Computes gradients and updates weights.\n",
    "* `train`: Runs the training loop with forward and backward passes.\n",
    "\n",
    "### When to use which loss:\n",
    "\n",
    "* Cross-entropy: Use for classification (e.g., Iris, MNIST) where outputs are probabilities.\n",
    "* MSE: Use for regression where outputs are continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import Layer\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all layers\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, loss_type='cross_entropy'):\n",
    "        # Compute loss\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        if loss_type == 'cross_entropy':\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "        elif loss_type == 'mse':\n",
    "            loss = np.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_pred, y_true, learning_rate, loss_type='cross_entropy'):\n",
    "        # Backward propagation\n",
    "        if loss_type == 'cross_entropy':\n",
    "            # For softmax + cross-entropy, gradient is (y_pred - y_true)\n",
    "            delta = y_pred - y_true\n",
    "        elif loss_type == 'mse':\n",
    "            delta = 2 * (y_pred - y_true)  # Derivative of MSE\n",
    "\n",
    "        # Propagate gradient backward through layers\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta, learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate, loss_type='cross_entropy', verbose=True):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y_true=y, loss_type=loss_type)\n",
    "            losses.append(loss)\n",
    "            # Backward pass\n",
    "            self.backward(y_pred, y, learning_rate, loss_type)\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return class predictions (argmax for classification)\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.randint(0, 10, (10, 5))\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "target_column = 'E'\n",
    "\n",
    "X = df.drop(target_column, axis=1).values\n",
    "y = df[target_column].values\n",
    "\n",
    "y_unique = np.unique(y)\n",
    "\n",
    "y_one_hot = np.zeros((y.shape[0], len(y_unique)))\n",
    "for i, label in enumerate(y_unique):\n",
    "    y_one_hot[y == label, i] = 1\n",
    "y = y_one_hot\n",
    "y = np.argmax(y, axis=1)\n",
    "y_new = np.random.randn(y.shape[0])*10 // 6\n",
    "y_new = np.array([int(i) for i in y_new])\n",
    "\n",
    "np.sum(y_new == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 4: `utils.py`\n",
    "\n",
    "This file handles data loading, preprocessing, evaluation, and visualization.\n",
    "### Explanation:\n",
    "\n",
    "* `load_data`: Loads a CSV and optionally one-hot encodes the labels.\n",
    "* `preprocess_data`: Normalizes features and splits data.\n",
    "* `accuracy`, `confusion_matrix`, `precision_recall`: Evaluation metrics.\n",
    "* `save_model`, `load_model`: Save/load weights using NumPy.\n",
    "* `plot_metrics`: Visualizes training loss (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_path, target_column, one_hot=True):\n",
    "    # Load CSV with Pandas\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=[target_column]).values  # Features\n",
    "    y = data[target_column].values  # Labels\n",
    "\n",
    "    # One-hot encode labels if classification\n",
    "    if one_hot:\n",
    "        y_unique = np.unique(y)\n",
    "        y_one_hot = np.zeros((y.shape[0], len(y_unique)))\n",
    "        for i, label in enumerate(y):\n",
    "            y_one_hot[i, label] = 1\n",
    "        y = y_one_hot\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Accuracy for classification\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred == y_true_labels)\n",
    "\n",
    "def confusion_matrix(y_pred, y_true):\n",
    "    # Simple confusion matrix\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes))\n",
    "    for t, p in zip(y_true_labels, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def precision_recall(y_pred, y_true):\n",
    "    # Precision and recall per class\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    return precision, recall\n",
    "\n",
    "def save_model(model, filename):\n",
    "    # Save weights and biases using NumPy\n",
    "    params = [{'weights': layer.weights, 'biases': layer.biases} for layer in model.layers]\n",
    "    np.save(filename, params)\n",
    "\n",
    "def load_model(filename):\n",
    "    # Load weights and biases\n",
    "    params = np.load(filename, allow_pickle=True)\n",
    "    nn = NeuralNetwork()\n",
    "    for param in params:\n",
    "        layer = Layer(param['weights'].shape[0], param['weights'].shape[1])\n",
    "        layer.weights = param['weights']\n",
    "        layer.biases = param['biases']\n",
    "        nn.add_layer(layer)\n",
    "    return nn\n",
    "\n",
    "def plot_metrics(losses, title=\"Training Loss\"):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
