{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Neural Network\n",
    "A neural network consists of:\n",
    "\n",
    "* Layers: Each layer has weights, biases, and an activation function.\n",
    "* Forward Propagation: Computes the output by passing input through the layers.\n",
    "* Loss Function: Measures the error between predicted and true outputs (e.g., cross-entropy for classification, MSE for regression).\n",
    "* Backpropagation: Computes gradients of the loss with respect to weights and biases.\n",
    "* Gradient Descent: Updates weights and biases to minimize the loss.\n",
    "\n",
    "We’ll implement a fully connected (dense) neural network with customizable layers and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing the Code\n",
    "File 1: `layers.py`\n",
    "This file defines a Layer class to represent a single layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The Layer class initializes weights and biases randomly.\n",
    "* forward: Computes the output of the layer (linear transformation + activation).\n",
    "* backward: Computes gradients and updates weights/biases using the chain rule.\n",
    "* Activation functions are temporarily here; we’ll move them to activations.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation = activation  # Activation function (e.g., 'relu', 'sigmoid', 'softmax')\n",
    "        # Store intermediate values for backpropagation\n",
    "        self.input = None\n",
    "        self.z = None  # Pre-activation output\n",
    "        self.a = None  # Post-activation output\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass: X is the input (batch_size, input_size)\n",
    "        self.input = X\n",
    "        self.z = np.dot(X, self.weights) + self.biases  # Linear transformation\n",
    "        if self.activation == 'relu':\n",
    "            self.a = relu(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.a = sigmoid(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = softmax(self.z)\n",
    "        else:\n",
    "            self.a = self.z  # No activation\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        # Backward pass: delta is the gradient from the next layer\n",
    "        if self.activation == 'relu':\n",
    "            delta = delta * relu_derivative(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            delta = delta * sigmoid_derivative(self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Softmax derivative is handled in the loss function (cross-entropy)\n",
    "            pass\n",
    "\n",
    "        # Compute gradients\n",
    "        dW = np.dot(self.input.T, delta)  # Gradient w.r.t weights\n",
    "        db = np.sum(delta, axis=0, keepdims=True)  # Gradient w.r.t biases\n",
    "        dX = np.dot(delta, self.weights.T)  # Gradient w.r.t input (for previous layer)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        return dX\n",
    "\n",
    "# Activation functions (will be moved to activations.py later)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [[ 0. -1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1. -1.  0.]\n",
      " [-1.  0. -1.]]\n",
      "(5, 3)\n",
      "\n",
      "bias: [[1. 1. 1.]]\n",
      "(1, 3)\n",
      "\n",
      "X: [[ 0.  0. -1. -1.  1.]\n",
      " [-1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  0.]\n",
      " [ 0.  0. -1.  0. -2.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [-1.  0. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1.]\n",
      " [ 0. -2.  0.  0.  0.]\n",
      " [-1.  0. -1.  0. -1.]]\n",
      "(10, 5)\n",
      "\n",
      "z: [[2. 3. 0.]\n",
      " [2. 3. 2.]\n",
      " [1. 1. 1.]\n",
      " [3. 3. 1.]\n",
      " [4. 2. 3.]\n",
      " [1. 1. 1.]\n",
      " [3. 4. 2.]\n",
      " [5. 3. 2.]\n",
      " [3. 1. 1.]\n",
      " [3. 3. 3.]]\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.random.randn(5, 3) // 2\n",
    "bias = np.ones((1, 3))\n",
    "X = np.random.randn(10, 5) // 2 # (batch_size, input_size)\n",
    "z = np.dot(X, weights) + bias # Linear transformation\n",
    "print(f\"weights: {weights}\")\n",
    "print(weights.shape)\n",
    "print(f\"\\nbias: {bias}\")\n",
    "print(bias.shape)\n",
    "print(f\"\\nX: {X}\")\n",
    "print(X.shape)\n",
    "print(f\"\\nz: {z}\")\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[0 0 3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "delta = np.array([1, 2, 3])\n",
    "z = np.array([-1, 0, 1])\n",
    "delta = delta * relu_derivative(z)\n",
    "print(relu_derivative(z))\n",
    "print(delta)\n",
    "\n",
    "db = np.sum(delta, axis=0, keepdims=True)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, -1,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3]])\n",
    "x - np.max(x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
